{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mottasilvia/UCU-NLP/blob/main/PLN_Actividad_1_RedesNeuronales.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential # Import Sequential from tensorflow.keras\n",
        "from tensorflow.keras.layers import LSTM, GRU, SimpleRNN # Import recurrent layers from tensorflow.keras\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout # Import core layers from tensorflow.keras\n",
        "from tensorflow.keras.layers import Embedding # Import Embedding layer from tensorflow.keras\n",
        "from tensorflow.keras.layers import BatchNormalization # Import BatchNormalization from tensorflow.keras\n",
        "from tensorflow.keras.utils import to_categorical # Import to_categorical for one-hot encoding\n",
        "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
        "from tensorflow.keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D # Import additional layers from tensorflow.keras\n",
        "from tensorflow.keras.preprocessing import sequence, text # Import preprocessing modules from tensorflow.keras\n",
        "from tensorflow.keras.callbacks import EarlyStopping # Import EarlyStopping from tensorflow.keras\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "from plotly import graph_objs as go\n",
        "import plotly.express as px\n",
        "import plotly.figure_factory as ff\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "8EHR_YVHCwHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar la disponibilidad de la GPU\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "if tf.test.gpu_device_name():\n",
        "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
        "else:\n",
        "    print(\"Please install GPU version of TF\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hh7TSH6Ed2er",
        "outputId": "bb67e9a4-0310-4a7e-8a88-7dcf9c21b66b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n",
            "Default GPU Device: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqjQlE90bmXL",
        "outputId": "37b4418c-4f6d-4d91-82f8-f31b0fc99d22"
      },
      "source": [
        "# Detect hardware, return appropriate distribution strategy\n",
        "try:\n",
        "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
        "    # set: this is always the case on Kaggle.\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Running on TPU ', tpu.master())\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "else:\n",
        "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "REPLICAS:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LATMW4Fyb5IM",
        "outputId": "c19a335f-e93a-497c-bed2-6c6a1a6641b2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "carpeta_laboratorios = \"/\"\n",
        "path_laboratorios = '/content/drive/My Drive/'\n",
        "#df = pd.read_csv(path_laboratorios+\"Canciones_limpias_finalisismo.csv\")\n",
        "\n",
        "#train = pd.read_csv(path_laboratorios+\"train.csv\")\n",
        "#test = pd.read_csv(path_laboratorios+\"test.csv\")\n",
        "#validation = pd.read_csv(path_laboratorios+\"val.csv\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "def unzip_to_folder(zip_files, target_dir=\"unzipped_files\"):\n",
        "  os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "  for zip_file in zip_files:\n",
        "    filename, _ = os.path.splitext(os.path.basename(zip_file))\n",
        "    folder_name = target_dir\n",
        "    os.makedirs(folder_name, exist_ok=True)\n",
        "\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "      zip_ref.extractall(folder_name)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  # Example usage\n",
        "  zip_files = [\"/content/drive/MyDrive/UCU-TesisFinal/aarchive (3).zip\"]\n",
        "  unzip_to_folder(zip_files, \"/content/Movie\")\n"
      ],
      "metadata": {
        "id": "_WUBOSyMEn9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "# Define los paths a los directorios que contienen las reseñas\n",
        "neg_dir = '/content/Movie/txt_sentoken/neg'\n",
        "pos_dir = '/content/Movie/txt_sentoken/pos'\n",
        "\n",
        "# Función para leer reseñas de un directorio y asignar una etiqueta\n",
        "def load_reviews_from_directory(directory, tag):\n",
        "    reviews = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
        "                review = file.read()\n",
        "                reviews.append((review, tag))\n",
        "    return reviews\n",
        "\n",
        "# Verificar si los directorios existen\n",
        "if os.path.exists(neg_dir) and os.path.exists(pos_dir):\n",
        "    # Cargar reseñas negativas y positivas\n",
        "    negative_reviews = load_reviews_from_directory(neg_dir, 'negative')\n",
        "    positive_reviews = load_reviews_from_directory(pos_dir, 'positive')\n",
        "\n",
        "    # Combinar las reseñas en un único DataFrame\n",
        "    all_reviews = negative_reviews + positive_reviews\n",
        "    df = pd.DataFrame(all_reviews, columns=['review', 'tag'])\n",
        "\n",
        "    # Mostrar el DataFrame\n",
        "    from IPython.display import display\n",
        "    display(df)\n",
        "\n",
        "    # Verificar las primeras filas del DataFrame\n",
        "    print(df.head())\n",
        "else:\n",
        "    print(\"Los directorios especificados no existen. Verifica los paths y vuelve a intentarlo.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "id": "TjjPOysInDBS",
        "outputId": "e25988b0-ae57-408b-e48e-753df9b848c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                                 review       tag\n",
              "0     i have nothing against unabashedly romantic fi...  negative\n",
              "1     in 1989 , director edward zwick began his care...  negative\n",
              "2     around the end of 1998 , a japanese cartoon ca...  negative\n",
              "3     the main problem with martin lawrence's pet pr...  negative\n",
              "4     what would inspire someone who cannot write or...  negative\n",
              "...                                                 ...       ...\n",
              "1995  it was a rainy friday afternoon in columbus wh...  positive\n",
              "1996  dreamworks pictures presents a jinks/ cohen co...  positive\n",
              "1997  plot : derek zoolander is a male model . \\nhe ...  positive\n",
              "1998  susan granger's review of \" osmosis jones \" ( ...  positive\n",
              "1999  until i saw the night of the hunter , it had b...  positive\n",
              "\n",
              "[2000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-38950214-897c-4f83-a763-24133649f793\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>i have nothing against unabashedly romantic fi...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>in 1989 , director edward zwick began his care...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>around the end of 1998 , a japanese cartoon ca...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>the main problem with martin lawrence's pet pr...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>what would inspire someone who cannot write or...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>it was a rainy friday afternoon in columbus wh...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>dreamworks pictures presents a jinks/ cohen co...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>plot : derek zoolander is a male model . \\nhe ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>susan granger's review of \" osmosis jones \" ( ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>until i saw the night of the hunter , it had b...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-38950214-897c-4f83-a763-24133649f793')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-38950214-897c-4f83-a763-24133649f793 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-38950214-897c-4f83-a763-24133649f793');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3d3de90c-8af8-4e99-b0d1-1bdbc8143edf\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3d3de90c-8af8-4e99-b0d1-1bdbc8143edf')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3d3de90c-8af8-4e99-b0d1-1bdbc8143edf button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_23d00ad3-1fb9-4397-97a9-adc7a19f5177\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_23d00ad3-1fb9-4397-97a9-adc7a19f5177 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 2000,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2000,\n        \"samples\": [\n          \"anastasia contains something that has been lacking from all of the recent disney releases . . . \\n ( especially hercules ) . . . \\nemotion . \\nall the wacky characters voiced by celebrities and fantastically animated adventure sequences aren't going to hold anyone's interest unless there is an emotional core to hold it all together . \\nnot since disney's beauty & the beast has there been such a compelling animated film with interesting characters and drama that works . \\nthe story of the romanov family , the rulers of russia , and their downfall begins the film . \\nanastasia , one of the daughters , narrowly escapes the mad monk rasputin ( voiced by christopher lloyd ) with her grandmother ( voiced by angela landsbury ) . \\nbut anastasia gets lost , and grows up with no memory of her royal origins . \\nthe grandmother lives in paris , hoping to someday find her beloved anastasia , the only member of the family to survive the russian revolution . \\na young woman named anya ( voiced by meg ryan ) runs across dimitri ( john cusack ) and vlad ( kelsey grammar ) , who are trying to put together a scheme to create an anastasia to fool the grandmother and get the reward money . \\nthey pick anya to pose as their princes , and of course , it is soon revealed that anya is in fact the lost anastasia . \\ndimitri realizes that he can never win her love as he is a commoner , and meanwhile , rasputin is not dead , and plots his revenge against the last romanov . \\nthe plot is actually quite well structured . \\nthere are a lot of complexities that may have to be explained to the kids later , but it moves along fast enough that they'll never have time to be bored . \\nwhat this really means is that the adults won't be bored by it either . \\nthere is a great deal of banter between anya and dimitri that is very funny , and their relationship develops quite naturally as the film progresses . \\nthere is , of course , lots of comedy for the kids , including rasputin's pet bat , bartok ( hank azaria ) , who is easily the funniest thing in the film . \\nhowever , the comical scenes never distract from the drama , but are worked into the plot almost seamlessly . \\nthe animation is gorgeous - the characters seem to come to life through the talented animators . \\nnot since beauty & the beast have animated characters had so much life to them . \\neven without the voices , they act . \\nthe musical number in paris is a show-stopper , with some of the backgrounds rendered in an impressionist painting style . \\nas with all recent animated features , there are songs , however , these songs do more than just provide fodder for top-40 singers to get on the radio ( although there are three of them during the end credits ) . \\nthe songs are all very catchy , and advance the plot , instead of just being showpieces ( except for the paris number , but that's so much fun , it's okay . . . ) . \\ni can't reccomend anastasia highly enough . \\nit's a wonderful film that ranks right up there with other animated classics . \\nkids and adults alike will enjoy it , and it's also nice to have a quality animated feature film from another studio besides disney . \\n\",\n          \"the characters in jonathan lynn's \\\" the whole nine yards , \\\" yet another in an endless recent string of mob comedies ( 1999's \\\" analyze this \\\" and \\\" mickey blue eyes , \\\" 2000's \\\" gun shy \\\" ) , are rarely ever written as real people , but merely as one-dimensional caricatures . \\nwhen an added layer to one of their personalities is revealed , it is not to service the character development , or to naturally offer up instinctive characterizations , but to accommodate the convoluted plot . \\nin a lightweight comedy , you might say , it is not required to have perfectly realized figures , just as long as they get the job done . \\nunfortunately , for a comedy to work , it has to at least succeed at being funny , and if there was a laugh-o-meter available , i'd guess that it briefly ascended around 2 . 5% of the time for me . \\nsuffice to say , the comedy in \\\" the whole nine yards \\\" works about as well as the broken-down lawnmower in my backyard , and when a giggle surprisingly arises every twenty to thirty minutes , it is by sheer luck . \\nnicholas \\\" oz \\\" oseransky ( matthew perry ) is an amateur dentist living in the quiet suburbs of quebec . \\ntrapped in a hateful marriage with the conniving sophie ( rosanna arquette , delightfully hamming it up with a clearly artificial french-canadian accent ) , one day oz sees that someone is moving into the house next-door . \\nwalking over to greet him , he is horrified to discover it is jimmy \\\" the tulip \\\" tudeski ( bruce willis ) , a former contract killer for the mafia who has just been released from prison . \\nonce getting to know him and becoming his friend , oz is more comfortable with his identity , but for sophie , it means a possible hitman to do away with oz . \\nin her scheme , sophie sends oz to chicago to cash in by informing his old mob boss ( kevin pollak ) about jimmy's whereabouts , but in the process , he falls in love with jimmy's wife , cynthia ( natasha henstridge ) , who is being held hostage . \\nupon return to quebec , and with one contrivance after the next , oz's dental assistant , jill ( amanda peet ) , is overjoyed to discover that he lives beside \\\" the tulip , \\\" and forces him to set up a meeting , since jill's dream in life turns out being a contract killer herself . \\nhave you got all that ? \\nbecause there is more . \\nmuch , much more . \\nand this is a movie that , without credits , is little over 90 minutes and feels only like an hour . \\n \\\" the whole nine yards \\\" is an ultimately unsatisfying and empty-headed excursion into well-worn terrain already set by far superior pictures . \\nhow the cast , most of which are respectable actors , got caught up in such a cliched , deficient film remains a mystery , unless they thought it might aspire to match the congenial screwball zaniness of director lynn's 1985 comedy classic , \\\" clue : the movie . \\\" \\ngoing into the theater , preliminary comparisons between the two movies were unavoidable , but by the twenty-minute mark , when i had chuckled once and laughed nary a single time , it was clear this production was in serious trouble . \\nsince the entire running time depends on the mechanisms of the plot , and the screenplay , inauspiciously written by mitchell kapner , collapses with every failing \\\" comedy \\\" bit , the film is an inevitable dead zone in the way of substance and , frequently , entertainment value . \\n \\\" the whole nine yards \\\" is neither facetious nor , aside from the subplot about jill's shocking eagerness to become a professional hitwoman , inventive , and more often than not , just lies there , the film spinning drearily around and around in the projector , but never igniting any sort of spark . \\na few select actors do what they can with the material , while others make no impact at all . \\non the mediocre side are actually those playing the two central characters : matthew perry and bruce willis . \\nperry , innocuously enjoyable on tv's \\\" friends , \\\" plays the same exact sitcom-style character in all of his movies ( from 1997's \\\" fools rush in \\\" to 1999's \\\" three to tango \\\" ) , and it has become a crushing bore . \\nmeanwhile , willis makes next to no impression , and because of the limited guise of his jimmy \\\" the tulip \\\" tudeski , oftentimes disappears into the background . \\ntheir three female counterparts fare noticeably better . \\nbest of all is natasha henstridge , far more radiant than in the exploitative \\\" species \\\" movies , who adds unanticipated depth and emotion to cynthia . \\nhenstridge has the talent , for sure , to break out of these countless throwaway roles , but first she must fire her agent . \\namanda peet has a lot of fun as the quirkily straightforward and giddy jill , and seems to know more about the art of comedic payoff than even perry who , thus far , has strived on a career based solely on comedy . \\nit is too bad , then , that peet is unnecessarily asked to disrobe in a climactic scene , with the obvious sole purpose being to show off her breasts . \\nfinally , rosanna arquette is awful as sophie , but something tells me that was her purpose , and her clear overacting only aids in brightening up her limited screen time . \\nalso popping up is michael clarke duncan , fresh off an oscar nomination for his role in \\\" the green mile , \\\" as jimmy's largely built friend and fellow killer , frankie figs . \\nwhen \\\" the whole nine yards \\\" eventually sputters to its underwhelming conclusion , one is left pondering how such a film ever got greenlit . \\na great deal of movies of this type have been made in the past , and this one is nothing but a duplication of better films , so what was the point ? \\nwithout a passable screenplay or any notable technical accomplishments , \\\" the whole nine yards \\\" rests solely on the presumed charm of the cast , and half of the actors are not charming at all . \\nnow , what does that tell you ? \\n\",\n          \"the ring is probably one of the creepiest movies i've seen in years . \\nbut then again , i've always had this phobia of asian born horror films . \\ni remember when i was a kid i was scared as hell with those green faced , long-haired floating ghosts donning their white gowns\\u0005\\u0005euuugh . \\nthe ring really brings back those memories and its grips me long after the movie has ended . \\nunlike the horror films coming from the west , i'm pretty much used to the gore and visual effects , so much so , they become rather bland nowadays . \\nthere's really no scary movie from the west for a long time . \\nif you have something in mind , please tell me . \\nthis japanese flick is supposed to be based upon stories written by `the stephen king of japan' , which is also ( i heard ) televised over the air in japan . \\nthe film revolves around a mysterious videotape which will bring death for sure to the person who watches it , one week prior to viewing . \\na journalist and her husband , stupid enough , tests the so call `urban legend' and ends up having a week before she meets her doom . \\nshe and her husband then goes on a search for the origin of the videocassette , to get to the root of the whole curse with the hopes of freeing themselves . \\nthe ring plays on the senses like no other horror film today . \\nvisual subtlety , minimal music and an intriguing plot puts the audience within an aura of suspense throughout the entire film . \\nvery well done indeed , especially for a film that does not have a single trace of violence and gore . \\nnow , that\\u0012s something budding horror film-maker wannabes should consider looking into . \\njust an idea of how visually powerful the film is , it is in japanese and subtitled in chinese ; both of which , i can't understand at all ( ok , i had a friend who gave me a gist prior to watching , but he revealed very little ! ) . \\nthe twists in the plot , leading to an unforgettable climax ( will definitely leave an impression on the audience ) will have you at the edge of your seat . \\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tag\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"positive\",\n          \"negative\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              review       tag\n",
            "0  i have nothing against unabashedly romantic fi...  negative\n",
            "1  in 1989 , director edward zwick began his care...  negative\n",
            "2  around the end of 1998 , a japanese cartoon ca...  negative\n",
            "3  the main problem with martin lawrence's pet pr...  negative\n",
            "4  what would inspire someone who cannot write or...  negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Dividir el DataFrame en train (60%) y temp (40%)\n",
        "train_df, temp_df = train_test_split(df, test_size=0.4, random_state=42, stratify=df['tag'])\n",
        "\n",
        "# Dividir el temp_df en validation (50% de 40% -> 20%) y test (50% de 40% -> 20%)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['tag'])\n",
        "\n",
        "# Mostrar los tamaños de los conjuntos de datos\n",
        "print(f\"Tamaño del conjunto de entrenamiento: {len(train_df)}\")\n",
        "print(f\"Tamaño del conjunto de validación: {len(val_df)}\")\n",
        "print(f\"Tamaño del conjunto de prueba: {len(test_df)}\")\n",
        "\n",
        "# Mostrar las primeras filas de cada conjunto de datos para verificación\n",
        "print(\"Conjunto de entrenamiento:\")\n",
        "print(train_df.head())\n",
        "\n",
        "print(\"\\nConjunto de validación:\")\n",
        "print(val_df.head())\n",
        "\n",
        "print(\"\\nConjunto de prueba:\")\n",
        "print(test_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GW58VTUiqNb8",
        "outputId": "8e1dd609-ebf8-4468-be62-011fcbbe0b33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño del conjunto de entrenamiento: 1200\n",
            "Tamaño del conjunto de validación: 400\n",
            "Tamaño del conjunto de prueba: 400\n",
            "Conjunto de entrenamiento:\n",
            "                                                 review       tag\n",
            "1523  many people dislike french films for their lac...  positive\n",
            "1202  ingredients : james bond , scuba scene , car c...  positive\n",
            "468   i have a confession . \\neven though i am a mov...  negative\n",
            "578   i didn't come into city of angels expecting gr...  negative\n",
            "1029  the relaxed dude rides a roller coaster \\nthe ...  positive\n",
            "\n",
            "Conjunto de validación:\n",
            "                                                 review       tag\n",
            "1152  meet joe black ( reviewed on nov . 27/98 ) \\ns...  positive\n",
            "1762  there must be some unwritten rule that states ...  positive\n",
            "1142  there's a moment in schindler's list when a nu...  positive\n",
            "612   nearly every film tim burton has directed has ...  negative\n",
            "202   retrospective : city of the living dead ( 1980...  negative\n",
            "\n",
            "Conjunto de prueba:\n",
            "                                                 review       tag\n",
            "200   when i originally saw the trailer for \" analyz...  negative\n",
            "1702  leonardo decaprio ( what's eating gilbert grap...  positive\n",
            "1209  as i write the review for the new hanks/ryan r...  positive\n",
            "1451  kirk douglas is one of those rare american act...  positive\n",
            "1409  i guess it's a credit to jackie chan and the g...  positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lX8PvSfgpc_1"
      },
      "source": [
        "# **Redes Neuronales Recurrentes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OC_wtAZ_osQV"
      },
      "source": [
        "Veamos el máximo de palabras por review para tener una idea luego a la hora de paddear y todas esas cosas.-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjsIc7QXe9u6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d513aa71-d835-4e11-e1ce-c648ac415127"
      },
      "source": [
        "max_words_per_review = df['review'].apply(lambda x: len(str(x).split())).max()\n",
        "max_words_per_review"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2678"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78ZBJvU8y3ei"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x_m__V5yrjMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dv7p-XHpzTFq"
      },
      "source": [
        "max_len = 2700 #Si bien el máximo es 2678, ponemos 2700 para tener un poco más de holgura. Se va a usar después para padear!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFSFwIz0ylXC"
      },
      "source": [
        "xtrain = train_df.review.values\n",
        "xvalid = val_df.review.values\n",
        "xtest = test_df.review.values\n",
        "ytrain = train_df.tag.values\n",
        "yvalid = val_df.tag.values\n",
        "ytest = test_df.tag.values\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xtrain\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6LD_n0jmv3-",
        "outputId": "cad3649d-961c-4d68-b089-d4ec0a19f7dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['every once in a while a movie comes along that completely redefines the genre : with dramas , it was citizen kane , with arthouse it was pulp fiction , and with comedy it was , well , that jim carrey guy ( okay , so he\\'s not a movie , but he did have a huge influence on the genre . \\nnot to mention an expensive one . ) \\nsometimes a movie even combines them all into a big , sprawling motion picture event , as did forrest gump four years ago . \\nwith action films , it was aliens , whic was released to much hype seven years after it\\'s equally-innovative parent , alien ( 1979 ) . \\ndirected and written by james cameron ( t2 : judgement day , the abyss , true lies ) , the authority on action films , it was a masterful encore to his sci-fi thriller the terminator ( 1984 ) . \\nwhile the original alien film was a dark , enclosed horror film that featured one alien slowly massacering a horrified crew , james cameron took the big-budget action film with aliens , which featured multiple aliens doing basically the same thing , although on a much-larger scale . \\nand boy , did he take that route ! \\ni\\'d say at about 165 mph or so . . . \\nthe film opens 57 years after the original , with lt . ripley ( weaver ) being found in her ship in a cryogenic state by a salvage vessel . \\nif you\\'ll recall , at the end of alien ripley , the only surviving member , cryogenically \" hibernated \" herself after expelling the rogue alien from her ship . \\nunfortunately , she thought she\\'d only be out for a couple of weeks . . . \\nonce she\\'s returned to earth , ripley is quickly interrogated by \" the company \" , who quickly dismiss her and her stories as lunacy . \\nin truth , they believe her , as they soon approach ripley with an offer to travel with some marines to a new colony planet as an \" alien advisor \" . \\nit seems that the colony planet was a once-breeding ground for the nasty aliens , and now all communication with the planet has been lost . . . \\nit doesn\\'t exactly take a genius to guess what happens next : ripley agrees , and before you can say \" big mistake \" , she and the half dozen marines , plus the slimy corporate guy ( reiser ) , who has more than it looks like up under his sleeve , are off to the colony . \\nwhen they arrive , they find the planet in ruins . \\nonly one survivor is found , a little girl , newt , who confirms that , yes , the aliens were here and that she only managed to survive by hidding in the ventilation system . \\nand soon enough , the marines come under attack from the aliens . . . \\nwhat happens for the next hour and a half or so is what completely sets this movie apart from any other standard alien sci-fi movie : the action scenes . \\ncameron directs them so skillfully , and so suspensefully , that you\\'re literally ringing your hands by the time the finale rolls around . \\nwhich features , in my opinion , the best fight scene ever recorded on film , as ripley straps herself into a huge robot and battles the nasty queen alien to the death . \\nmany people will tell you that this film , while being a great action film , has no real drama and is all cliches . \\nwell , they would be wrong , my friends . \\nif this film had no \" drama \" , then why was sigourney weaver nominated for best actress at the 1987 academy awards ? \\nthat\\'s right , best actress . \\nyou know that any action film that has an oscar nomination attached to it for something other than technical stuff like editing and f/x has got to be good . \\nin short , aliens combines all the right elements ( great action and f/x , drama , a good plot , good dialogue , and great villains ) into what could arguably be called the best action film of all time . \\nthen again , maybe not . \\nmovies rise and fall from glory and , sad to say , aliens was wrestled from it\\'s throne of best action movie by another cameron film , t2 : judgement day , in 1991 . \\nso who will be the next king ? \\nwell , let\\'s wait until december 19th and see yet another james cameron film-the highest budgeted film of all time-titanic to make that decision . \\ni can\\'t wait . \\n',\n",
              "       ' \" gattaca \" represents a solid breakthrough in the recent onslaught of science-fiction films -- it\\'s a genre picture that doesn\\'t rely on alien creatures or loud explosions to tell its story . \\nthe movie takes place in a futuristic world where babies are created through genetic tampering and not sexual reproduction . \\nthis allows parents to predetermine what kind of eye color , intelligence and life span they\\'d like for their child , and also eliminates most pesky chances of health defects . \\nthose made the old-fashioned way are labeled as \" in-valids \" and confined to the lower rung of society . \\nvincent freeman ( ethan hawke ) is one such person , born not too long before the genetic process was perfected and forced to grow up in a home with his petrie dish-molded younger brother anton . \\nfed up with being second-rate and enchanted by dreams of one day traveling through space , vincent leaves home and takes a janitorial position at the gattaca aerospace corporation . \\neveryday , he watches as \" superior \" folk make his fantasy a reality . \\ndetermined to do the same , vincent meets a dna broker ( tony shalhoub in a funny cameo ) who sells fake identities to in-valids . \\nhis counterpart for this scam is jerome morrow ( jude law ) , an ex-athlete left paralyzed in an accident and confined to a wheelchair for life . \\nfor a price and the promise of a caretaker , jerome supplies vincent with his identity , as well as blood , skin and urine samples for all of those pesky on-the-job tests and physical examinations -- this future is one where employees clock in by pricking their fingers instead of punching a time card . \\nbecause of his drastically improved status , vincent is quickly propelled to a high position in gattaca , and catches the eye of comely co-worker irene ( uma thurman ) on the way -- obsessed with her own minor heart defect , she\\'s enamored by his flawless persona she doesn\\'t know is a lie . \\nbut on the figurative eve of his upcoming planetary departure , the mission director is murdered . \\ntwo ardent detectives ( alan arkin and billy bathgate\\'s loren dean ) determine the killer is on the inside of gattaca , and the sole clue they find at the crime scene -- one of vincent\\'s eyelashes -- threatens to blow vincent\\'s cover and derail his goal . \\neven if \" gattaca \" were dramatically empty , it would still boast a sublime set of production credentials -- the film\\'s look is dazzling without ever being flashy . \\nfirst-time director/writer andrew niccol ( he also wrote the screenplay for the upcoming jim carrey drama \" the truman show \" ) demonstrates a keen eye for the stylish ; his collaboration with cinematographer slawomir idziak , production designer jan roelfs and costume designer colleen atwood result in a sophisticated composition that emanates classy , retro ambiance . \\nan opening credits sequence -- where skin , nail and hair fragments fall in slow-motion through a colored camera lens -- displays these combined talents uncannily . \\nniccol even utilizes voice-over narration ( a device almost always poorly-employed ) in an effective way . \\nwhen it comes to acting , the movie is also flawless . \\nethan hawke does magnificent work , proving his ability to carry a film and reaffirming his enormously high charisma level . \\nhis chemistry with thurman is a bit on the icy side , but needfully so , adding to the setting\\'s clinical chill . \\nas the bitter jerome , jude law has a star-making presence , and it\\'s his scenes with hawke that give the movie its fine emotional core . \\nin \" gattaca \" \\'s series of final shots , the fates of vincent and jerome are superimposed over each other , and the effect is sad , lyrical and beautiful . \\nthings get a little strange as the movie nears its climax , when vincent\\'s relationship with his brother comes back into view . \\nthe big dramatic culmination is a swim race , which is somewhat silly albeit beautifully photographed . \\nstill , the single most surprising aspect of gattaca is its use of backdrop -- it\\'s successful sci-fi without showy special effects , it\\'s a crisp thriller with character-driven thrills , it\\'s a futuristic fable without blood and guts . \\neven its murder mystery is relegated to a secondary subplot , ensuring that a smart story about smart people and smart science takes center stage . \\nalthough it\\'s portrayal of the upcoming century is grim , \" gattaca \" serves up one of the most thought-provoking societal forecasts ever depicted on film . \\n',\n",
              "       \"at the outset of swordfish , john travolta's gabriel shear is pontificating about the status of american cinema today . \\nbasically , he says , it boils down to a lack of imagination among the majority of writers . \\nhow ironic , as travolta seems to be describing his latest venture . \\nswordfish is loud , violent and amoral . \\nit has the audacity to justify murder and mayhem in the name of sustaining our way of life . \\nand how does travolta's gabriel plan to do this ? \\nby robbing billions from his own government and using the funds to out-terrorize terrorists . \\nswordfish is a very cynical movie . \\nit relies on an audience's perception of our leaders as ineffectual and duplicitous and on terrorists as non-human , faceless entities not worthy of compassion or consideration . \\nthe movie's plot is preposterous with enough illogical leaps that if the film ever slowed down , you'd actually see how ridiculous it all is . \\nthis is a live-action road runner cartoon , moving so quickly that it's over before you can catch your breath to ask any reasonable questions . \\nthe storyline revolves around super hacker stanley jobson ( hugh jackman ) , recruited by gabriel to crack the government's computer codes so gabriel can gather billions for his anti-terrorist campaign . \\ntalk about whacked-out patriotism . \\nmy objections to swordfish are many . \\nthe body count is high , but that is expected in a movie of this sort . \\nit's becoming a bore watching anonymous soldiers , police officers and government agents blown to bits . \\nanother example is the family dynamics between stanley , his 10-year-old daughter and his ex-wife . \\nstanley , though having served time in prison for hacking , is shown as a loving and caring father , forbidden by his ex to see his little girl . \\naudience animosity is immediately created for his former spouse by showing her as a drinker and smoker who also sometimes stars in her new husband's adult films . \\nthus when she is found murdered late in the movie , neither stanley nor his daughter are allowed any time to grieve . \\nin fact , subconsciously , many in the audience are probably glad she was killed . \\nthen there is the sequence involving one of gabriel's henchman holding a gun to the head of stanley's daughter to coerce the hacker to download the key computer program for gabriel . \\nchildren as pawns have become a most unwelcome clich ? in recent films . \\nthere is enough violence in the real world involving children without having to make them on-screen victims as well . \\nyea , it's only make believe , but that doesn't mean you have to tolerate it . \\ntravolta is cool , deadly charming and flamboyant as the near-crazy gabriel . \\nhis character is reminiscent of his villainous characterizations in broken arrow and face/off . \\njackman looks dour through most of the proceedings . \\nhis only moment of any depth comes when he finally is able to create the worm to get inside the government database . \\nhis sense of joy and accomplishment is one any computer whiz can appreciate . \\nhalle berry is decorative and lovely as gabriel's assistant , while don cheadle is given little to do as the head fbi agent hunting gabriel . \\nswordfish plays like a comic book with a larger-than-life character in gabriel . \\nviewers align themselves with him despite their uncertainty if he is hero or villain . \\nand maybe that is the movie's underlying flaw : there is no real hero to speak of , only those doing their upmost to survive . \\nand that is not enough . \\nthis is one swordfish that should have been thrown back in the water . \\n\",\n",
              "       ...,\n",
              "       'every year--every year at the festival , i wait for that film to come along , that one that just pulls me out of my seat , sticks its face up next to my nose , and roars \" sur-prise ! \" \\ninto my bewildered visage . \\nit\\'s almost always a surprise . \\nit sure as niflheim was this time . \\namazing grace and chuck is being advertised as a modern fairy tale , of a boy in montana who quits his little league team for a very unusual reason . \\nand , in the hands of anyone less careful than the creative staff of this film , it might very well be nothing more than a fairy tale , where we roll our eyes occasionally , smirk to ourselves , and maybe get a forced tear out of the eyes and a \" boy , i wish that could happen \" sigh out of the lips upon exiting the theater and tossing the empty pepsi cup into the trash . \\nanother e . t . \\nanother short circuit . \\nthis film floored me , for the simple reason that while it has a fairy tale concept , the rest of the film takes itself seriously enough , and presents itself well enough , to make it more of an american folk tale , with characters who are both icons and real people at the same time . \\namerica has always had its mythical heros , its paul bunyans and john waynes ; this film presents us with more general , but still universal , ideals : the honest , innocent children who have their own inner wisdom ; the athletes who seem to be amalgamations of courage , honor , and love for their respective sport ; the venerable elected official who leads with kindness and understanding , but has the grit to get things done when they need doin\\' ( does the latter sound familiar ? ) . \\namazing grace and chuck is a showcase for these characters , but it never leaves you with the feeling that it\\'s artificial , that it stands behind glass , or that any sharp breeze--or , more importantly , sharp thought--will shatter the wax facade of the panorama . \\nthis is a very sturdy scenario . \\nthe principals are always given dialogue , and always give performances--always ( it just blows me away ) -- which made them seem real , yet enforces their particular mythic role . \\nthe writer/producer , david field , seems to literally take all the \" yeah , but in real life , this would have happen \" thoughts you get in your head , sticks them in the movie and uses them to bend the plot around to his original heading , in a stronger way then before ! \\nastounding ! \\nhe uses obstacles to the plot to * enforce * it ! \\ni am truly impressed ( indeed , envious ) with the skill in which he wrote the story and screenplay ; it\\'s so very unusual , especially in a hollywood film . \\ni don\\'t want to give too much away , but the basic premise is that chuck ( joshua zuehlke ) , the little league pitcher , decides to give up baseball because of nuclear weapons . \\nhis decision begins an unlikely series of events that involve another athlete , a boston basketball player ( alex english ) , \" amazing \" grace smith , and , well , i\\'m leaving it at that because i wouldn\\'t spoil this film for you for the world . \\nlet me just say this , though : i am not recommending this film because i think it has a great message or because of any political positions it might imply . \\ni don\\'t give a rat\\'s ass for the political point-of-view this film expresses , one way or another ; i\\'m recommending you go see this film because , and only because , it\\'s an excellent story , told with excellence . \\nno , i don\\'t believe what happens in this film could happen in real life ; while i tend to believe the arms control policy of this country is stilted , i believe in careful negotiations , mutual verification , etc . screw what i think . \\nthe point is , this film is able to suspend my disbelief and tell a story that is one of the most finely crafted pieces of american dream i have ever seen on the movie screen . \\nthis is the natural and more--all the mythic qualities without the pretentiousness or the forced feeling of the conclusion , and a much better script to boot . \\nit carried me into the beliefs and ideals of my boyhood--and , more importantly , without any bumps or jolts that would snap me out of the trance with some hint of self-consciousness . \\nspecial kudos to : both zuehlke ( a real-life little league pitcher who was picked for the part ) and english ( a forward for the denver nuggets ) for their seamless personification of their characters ; jamie lee curtis , who takes a surprisingly small role and makes it exceedingly memorable as amazing\\'s manager and friend ; william l . peterson ( in a * big * change from his role in to live and die in l . a . ) as a father who shows principles without having to stand up and wave a flag doing it ; and gregory peck , as the guy we wish ronald reagan really was ( and who some numbskulls still probably think he is \\n',\n",
              "       'capsule : suprisingly more of a comedy than a straight action flick , which isn\\'t necessarily a bad thing . \\nnot exactly oscar caliber , but one helluva bullet-riddled good time . \\nextended review : you know , i remember when hitmen were evil , murderous scum . \\nalas , the times are a-changin\\' . \\nin a recent string of movies , hitmen are suddenly wise- cracking , fun-loving killers-with-hearts . \\nthis brings us to hong kong director kirk wong\\'s first american feature , the big hit . \\noddly enough , about the same time last year a similar film , grosse point blank , was released . \\nadvertised as a quirky comedy with hints of action , it turned out to have a suprising dosage of it . \\nthe big hit is quite the opposite . \\nit was hyped as \" the new film from producer john woo \" , so one would it expect lots of stylized killing and action . \\nhowever , there\\'s a sore lack of it , which is about the only thing wrong with the big hit . \\nthe film starts out with mel smiley and his cohorts doing a job on a white slaver . \\nmel , played by mark wahlberg in a dopey , milquetoast role , is a killing machine ; he flips , spins , even breakdances whilst popping caps . \\nsadly , he doesn\\'t get a chance to do much of it . \\nexcept for the beginning set piece and the last 20 twenty minutes or so , the film is in comedy mode . \\nthe action , at least what there is of it , is prime cut stuff . \\nwong , after numerous hong kong features , makes quite a nice u . s . debut . \\nhowever , his pacing is a bit off , with the action sequences only bookending the movie and not lasting long enough . \\nthey start off electrifying and fresh , but just kinda stop . \\nnormally , this would hamper a movie to the point of being unenjoyable . \\nluckily , we have ben ramsey\\'s screenplay , a bitingly funny piece of work . \\nthe only problem is there might be too much humor , one joke makes you laugh so hard you miss the next few . \\nsome of best gags include an oriental film maker down on his luck and one of mel\\'s hitmen pals that has just discovered onanism . \\nthe only problem is how some of the minor characters are handled , some being there only for a laugh , which sometimes works , sometimes doesn\\'t . \\noverall , the big hit may have it\\'s flaws , but it makes up for them in a stylishly directed , gut-wrenchingly funny joyride . \\ndefinately one of the better ways to spend two hours . \\n',\n",
              "       'i suppose an argument could be made that toy story is one of those films that didn\\'t need a sequel . \\nbeloved by kids and their parents , respected equally by mainstream america and geekish movie buffs , that first movie remains a landmark of recent history , the one that burst open the possibilities of computer animation and demonstrated through wild invention and giddy chutzpah just how complacent the disney animation machine had become in cranking out fluffy razzle-dazzle entertainment full of formula storytelling and banal songwriting . \\nif disney was embarrassed at being beaten at its own game ( toy story was a smash hit of unexpected proportions , and one that caught the merchandising end of the business unaware as demand for action figures far outstripped supply ) , it didn\\'t show , and as distributor and part-owner of the property , at least it had a piece of the action . \\ntoy story 2 was greenlighted as a direct-to-video project , disney\\'s standard tactic for milking a few more bucks out of hot franchises without expending the effort of developing a proper feature film . \\nas someone who doesn\\'t believe sequels are necessarily a bad thing ( granted , they usually are a bad thing , but that\\'s because they\\'re made for the wrong reasons ) , i had to wonder what in the world they were thinking . \\nfortunately , disney claimed to have been so knocked-out by early animation tests that they let pixar go full-speed ahead with a theatrical sequel . \\nlucky thing , too -- like the first movie , this one is a joy to behold on the big screen , and technically , it improves on its predecessor on just about every level . \\n ( from a business standpoint , the end credits show that the new creations are copyrighted by pixar , while the previous film\\'s elements are shared between pixar and disney , a sign of the production house\\'s new cachet in hollywood . ) \\nvisually , the main shortcoming of this fully computer-generated movie is that human figures are still rendered relatively poorly , making them look a little creepy . \\nfortunately , that eerie unreality fits in perfectly with the perspective of the movie , where the secret world of toys is more immediate , and arguably more attractive , than the world of the humans who surround them . \\nas before , the story is predicated on the premise that the toys scattered around the bedroom of a little boy named andy -- and , indeed , all the toys scattered around the bedrooms of all little children -- come to life in the child\\'s absence . \\nwhile the toys scamper about and chatter endlessly among themselves , the real joy of a toy\\'s life is to mean something to its owner . \\none of the subjects of this new film is the sadness of toys that have been broken or abandoned , left on a shelf to gather dust , and another is the sort of emotional limbo inhabited by toys that are mere prizes of covetous collectors . \\nmost often , those toys are packed away in dark spaces , safe from sunlight and humidity , and often they\\'re not even removed from their packaging . \\nimagine what a chip that would put on your shoulder , and you\\'ll understand the attitudes of the collector\\'s items that show up in this movie . \\nsheriff woody ( voiced by a note-perfect tom hanks ) , the longtime favorite among andy\\'s toys who was challenged in the previous go-round by the arrival of flashy-new-thing action figure buzz lightyear ( tim allen ) , suffers an injury early in the film , when andy tugs too hard on his arm and pulls a seam apart , revealing the stuffing inside . \\nthis accident catalyzes some uneasieness among the toys , who know too well that a broken toy is often a forgotten toy , and a forgotten toy is one that loses its reason for existence . \\nthose anxieties are crystallized when andy\\'s mom tears through his living room , collecting old toys for a yard sale . \\nand there\\'s an ironic twist to the tale , as woody winds up being stolen by an avid toy collector who needs exactly that quaint cowboy figure to complete a set that he hopes to sell to a japanese toy museum for a sizable sum . \\nthe rest of andy\\'s toys , who owe him quite a debt , resolve to rescue him . \\nthat this film manages to turn a box marked \" 25 cents \" into a symbol of doom , or to make its screed against the retention of collectible toys by wrongheaded profiteers fuel for a metaphysical dilemma , is a testament to its skill at metaphor , seamlessly translating the hopes and fears of our real world into that of the toys . \\noperating on this level of abstraction , toy story 2 tackles some mighty heavy issues without once preaching or veering into pretentiousness . \\n ( the worst i can say is that randy newman seems to have reserved his sappiest lyric in years for sarah mclachlan , who stops the movie cold by singing it at just about the halfway mark . ) \\nsometimes i think toy story 2 tries too hard . \\nthere\\'s somewhat less of the seat-of-the-pants loopiness that energized the first film , allowing it to surprise and excite on a near-constant basis , and more philosophizing about toys , collectors , the nature of happiness and the meaning of life . \\nwhile that leads to fewer bellylaughs , it does make way for more elaborate humor and an uncommonly ambitious reflexivity that asks the toys to consider their own status as commodities that move in and out of fashion . \\n ( just don\\'t ask why andy\\'s favorite toy is based on a tv series that was canceled in 1957 . ) \\nwhere else in mainstream movies do you get such an awesome moment as the one where buzz arrives at al\\'s toy barn to find it stocked to the gills with his doppelgangers , buzz lightyear action figures ? \\nforget the self-congratulatory science fiction of the matrix -- this is a fundamental mind-bender for buzz , and the audience shares his humility and wonderment at the sight . \\nhere , as in the roughly concurrent scene where woody watches tapes of the howdy doodyish children\\'s tv show that originated his character , we see our protagonists come face-to-face with god . \\nin sly ways , then , toy story 2 can be read as a film about mortality , a metaphorical consideration of aging and death . \\nsignificantly , the film\\'s very first sequence concludes with a grim shocker that had our opening-night crowd in a near-uproar . \\nand toward the end , when buzz and woody speculate on how long they have before andy grows up and discards his old toys , one of them observes , with an alacrity both inspirational and heartbreaking , that it will be fun while it lasts . \\nso toy story 2 joins the tradition of children\\'s stories , largely neglected of late , that say something real about the inevitable joys and tragedies of existence . \\nwhat\\'s really striking is that both toy story films ( and , to a lesser degree , pixar\\'s a bug\\'s life ) are kids\\' movies with wit and sophistication to shame most of their ostensibly adult counterparts , not to mention whatever piece of tot-friendly eye-candy is due from the disney dream factory any given summer . \\nit puts one in mind of the glory days of chuck jones and the old gang at warner bros . animation . \\ni\\'m not sure lasseter and his pals at pixar will ever operate at quite that level of purely visual invention -- they love traditional narrative too much -- but , boy , it makes me wonder what they might come up with next . \\n-------------------------------------------------------------- directed by john lasseter , colin brady , ash brannon , and lee unkrich written by lasseter , brannon , peter docter , andrew stanton , rita hsaio , doug chamberlain , and chris webb cinematography by sharon calahan starring ( voices ) tom hanks , tim allen , and joan cusack usa , 1999 \\ntheatrical aspect ratio : 1 . 85 : 1 -------------------------------------------------------------- \\n'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "xXCkmVlxsKo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ho1uGwLFfjm5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d4c7851-b44d-44e6-dc04-37dbc44e7992"
      },
      "source": [
        "# Usando el tokenizer de Keras\n",
        "tokenizer = Tokenizer(num_words=None)  # Puedes ajustar num_words si deseas limitar el tamaño del vocabulario\n",
        "\n",
        "# Ajustar el tokenizer a los textos de entrenamiento y validación\n",
        "tokenizer.fit_on_texts(list(xtrain) + list(xvalid))\n",
        "\n",
        "# Convertir los textos a secuencias de enteros\n",
        "xtrain_seq = tokenizer.texts_to_sequences(xtrain)\n",
        "xvalid_seq = tokenizer.texts_to_sequences(xvalid)\n",
        "xtest_seq = tokenizer.texts_to_sequences(xtest)\n",
        "\n",
        "# Aplicar padding a las secuencias\n",
        "xtrain_pad = pad_sequences(xtrain_seq, maxlen=max_len, padding='post')\n",
        "xvalid_pad = pad_sequences(xvalid_seq, maxlen=max_len, padding='post')\n",
        "xtest_pad = pad_sequences(xtest_seq, maxlen=max_len, padding='post')\n",
        "\n",
        "# Obtener el índice de palabras del tokenizer\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Verificar los resultados\n",
        "print(f\"Tokenizador ajustado con {len(word_index)} palabras únicas.\")\n",
        "print(f\"Ejemplo de secuencia paddeada (xtrain_pad[0]): {xtrain_pad[0]}\")\n",
        "print(f\"Forma de xtrain_pad: {xtrain_pad.shape}\")\n",
        "print(f\"Forma de xvalid_pad: {xvalid_pad.shape}\")\n",
        "print(f\"Forma de xtest_pad: {xtest_pad.shape}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizador ajustado con 39609 palabras únicas.\n",
            "Ejemplo de secuencia paddeada (xtrain_pad[0]): [ 112   99 7160 ...    0    0    0]\n",
            "Forma de xtrain_pad: (1200, 2700)\n",
            "Forma de xvalid_pad: (400, 2700)\n",
            "Forma de xtest_pad: (400, 2700)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xtrain_pad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QIfNxgRo34F",
        "outputId": "2d99c0de-961a-45e5-f7ce-120ece6163b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 112,   99, 7160, ...,    0,    0,    0],\n",
              "       [3862,  328,  942, ...,    0,    0,    0],\n",
              "       [  17,   30,    2, ...,    0,    0,    0],\n",
              "       ...,\n",
              "       [4326,    6,   76, ...,    0,    0,    0],\n",
              "       [  11,    2,  448, ...,    0,    0,    0],\n",
              "       [  17,  123,  120, ...,    0,    0,    0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "# Convertir etiquetas de texto a enteros\n",
        "label_encoder = LabelEncoder()\n",
        "ytrain_encoded = label_encoder.fit_transform(ytrain)\n",
        "yvalid_encoded = label_encoder.transform(yvalid)\n",
        "ytest_encoded = label_encoder.transform(ytest)\n",
        "\n",
        "# Convertir los enteros a representación one-hot\n",
        "ytrain_one_hot = to_categorical(ytrain_encoded)\n",
        "yvalid_one_hot = to_categorical(yvalid_encoded)\n",
        "ytest_one_hot = to_categorical(ytest_encoded)\n",
        "\n",
        "# Verificar las formas de los arrays one-hot\n",
        "print(f\"Forma de ytrain_one_hot: {ytrain_one_hot.shape}\")\n",
        "print(f\"Forma de yvalid_one_hot: {yvalid_one_hot.shape}\")\n",
        "print(f\"Forma de ytest_one_hot: {ytest_one_hot.shape}\")\n",
        "\n",
        "# Opcional: Verificar los primeros ejemplos de etiquetas one-hot\n",
        "print(\"Ejemplo de etiquetas one-hot (ytrain_one_hot[0]):\")\n",
        "print(ytrain_one_hot[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgJPgPIxsphI",
        "outputId": "fca27da8-eea7-458f-e98c-5312607d1b42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma de ytrain_one_hot: (1200, 2)\n",
            "Forma de yvalid_one_hot: (400, 2)\n",
            "Forma de ytest_one_hot: (400, 2)\n",
            "Ejemplo de etiquetas one-hot (ytrain_one_hot[0]):\n",
            "[0. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNxvBuDwhscW"
      },
      "source": [
        "En una RNN ingresamos una review palabra por palabra. Representamos cada palabra como un vector utilizando la técnica del one hot encoding. El vector va a tener cantidad de palabras en el vocabulario + 1 domensiones.\n",
        "\n",
        "Lo que hace keras Tokenizer es, toma todas las palabras únicas en el corpus, forma un diccionario con palabras como claves y su número de ocurrencias como valores (es el word_index), luego ordena el diccionario en orden descendente de conteos.\n",
        "\n",
        "Luego asigna a la primer palabra el valor 1, a la segunda el valor 2 y así sucesivamente.\n",
        "\n",
        "Así que supongamos que la palabra 'que' es la que se repite más en todas las canciones, a esa se le asignará el índice 1 y el vector que representa la palabra 'que' sería un vector one-hot con valor 1 en la posición 1 y resto ceros.\n",
        "\n",
        "La primera línea del modelo \"Sequential()\" le dice a Keras que construiremos nuestra red secuencialmente.\n",
        "\n",
        "Luego, primero agregamos la capa de incrustación. La capa de Embedding que es también una capa de neuronas que toma como entrada el vector one-hot n-ésimo de cada palabra y lo convierte en un vector de 300 dimensiones, nos da el enbeddubg de palabras similar a word2vec. Podríamos haber utilizado word2vec, pero la capa de Embedding aprende durante el entrenamiento para mejorar la forma de embeddear.\n",
        "\n",
        "A continuación, agregamos 100 unidades LSTM sin ningún dropout ni regularización.\n",
        "\n",
        "Por último, agregamos 5 neuronas (ya que tenemos 5 posibles clases) con función sigmoidea que toma la salida de las 100 células LSTM (tener en cuenta que tenemos 100 células LSTM, no capas) para predecir los resultados y luego compilamos el modelo usando adam optimizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd-FgV_L1xqr"
      },
      "source": [
        "# RNNs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ihmy2k5s1xYY",
        "outputId": "f67cfb4e-61ca-4bab-f6ed-0bc0e4a0aae0"
      },
      "source": [
        "%%time\n",
        "with strategy.scope():\n",
        "    # A simpleRNN without any pretrained embeddings and one dense layer\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(word_index) + 1,#Cantidad de palabras distintas del corpus\n",
        "                     300,#tamaño del vector con el que va a encodear cada palabra (del 1-hot lo pasa a un vector de 300 dimensiones)\n",
        "                     input_length=max_len)) #El máximo de palabras que puede tener una canción.\n",
        "    model.add(SimpleRNN(100))\n",
        "    model.add(Dense(50, activation='relu'))\n",
        "    model.add(Dense(20, activation='relu'))\n",
        "    model.add(Dense(2, activation='sigmoid'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 2700, 300)         11829000  \n",
            "                                                                 \n",
            " simple_rnn_1 (SimpleRNN)    (None, 100)               40100     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 50)                5050      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 20)                1020      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 2)                 42        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11875212 (45.30 MB)\n",
            "Trainable params: 11875212 (45.30 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "CPU times: user 400 ms, sys: 200 ms, total: 600 ms\n",
            "Wall time: 990 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(xtrain_pad, ytrain_one_hot, epochs=20, batch_size=64, validation_data=(xvalid_pad, yvalid_one_hot))\n",
        "\n",
        "# Predecir en el conjunto de validación\n",
        "preds = np.argmax(model.predict(xvalid_pad), axis=-1)\n",
        "\n",
        "# Calcular la precisión\n",
        "accuracy = metrics.accuracy_score(yvalid_encoded, preds)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Generar el reporte de clasificación\n",
        "target_names = label_encoder.classes_  # Esto debería ser ['negative', 'positive']\n",
        "clas_report = classification_report(yvalid_encoded, preds, target_names=target_names)\n",
        "print(clas_report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DLI2Ojovv1t",
        "outputId": "a763ca93-1f73-4e39-8400-1115dfb171c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "19/19 [==============================] - 111s 6s/step - loss: 0.7034 - accuracy: 0.4900 - val_loss: 0.6932 - val_accuracy: 0.5400\n",
            "Epoch 2/20\n",
            "19/19 [==============================] - 96s 5s/step - loss: 0.7021 - accuracy: 0.4858 - val_loss: 0.6980 - val_accuracy: 0.5000\n",
            "Epoch 3/20\n",
            "19/19 [==============================] - 122s 6s/step - loss: 0.6953 - accuracy: 0.5292 - val_loss: 0.7042 - val_accuracy: 0.5075\n",
            "Epoch 4/20\n",
            "19/19 [==============================] - 103s 5s/step - loss: 0.7001 - accuracy: 0.4942 - val_loss: 0.7030 - val_accuracy: 0.4725\n",
            "Epoch 5/20\n",
            "19/19 [==============================] - 104s 6s/step - loss: 0.6977 - accuracy: 0.4858 - val_loss: 0.7011 - val_accuracy: 0.5050\n",
            "Epoch 6/20\n",
            "19/19 [==============================] - 104s 5s/step - loss: 0.7002 - accuracy: 0.4908 - val_loss: 0.6988 - val_accuracy: 0.4700\n",
            "Epoch 7/20\n",
            "19/19 [==============================] - 107s 6s/step - loss: 0.7018 - accuracy: 0.4858 - val_loss: 0.6940 - val_accuracy: 0.5125\n",
            "Epoch 8/20\n",
            "19/19 [==============================] - 107s 6s/step - loss: 0.7047 - accuracy: 0.4883 - val_loss: 0.6886 - val_accuracy: 0.5300\n",
            "Epoch 9/20\n",
            "19/19 [==============================] - 100s 5s/step - loss: 0.7000 - accuracy: 0.5025 - val_loss: 0.7064 - val_accuracy: 0.4900\n",
            "Epoch 10/20\n",
            "19/19 [==============================] - 103s 5s/step - loss: 0.7016 - accuracy: 0.4850 - val_loss: 0.6944 - val_accuracy: 0.4800\n",
            "Epoch 11/20\n",
            "19/19 [==============================] - 104s 5s/step - loss: 0.6970 - accuracy: 0.4942 - val_loss: 0.6872 - val_accuracy: 0.5625\n",
            "Epoch 12/20\n",
            "19/19 [==============================] - 106s 6s/step - loss: 0.6977 - accuracy: 0.4908 - val_loss: 0.6966 - val_accuracy: 0.4825\n",
            "Epoch 13/20\n",
            "19/19 [==============================] - 110s 6s/step - loss: 0.6968 - accuracy: 0.5058 - val_loss: 0.6968 - val_accuracy: 0.4875\n",
            "Epoch 14/20\n",
            "19/19 [==============================] - 98s 5s/step - loss: 0.6963 - accuracy: 0.5042 - val_loss: 0.6944 - val_accuracy: 0.5325\n",
            "Epoch 15/20\n",
            "19/19 [==============================] - 98s 5s/step - loss: 0.6979 - accuracy: 0.4933 - val_loss: 0.6919 - val_accuracy: 0.5225\n",
            "Epoch 16/20\n",
            "19/19 [==============================] - 100s 5s/step - loss: 0.6978 - accuracy: 0.5042 - val_loss: 0.6972 - val_accuracy: 0.4850\n",
            "Epoch 17/20\n",
            "19/19 [==============================] - 101s 5s/step - loss: 0.6987 - accuracy: 0.4925 - val_loss: 0.7011 - val_accuracy: 0.4650\n",
            "Epoch 18/20\n",
            "19/19 [==============================] - 94s 5s/step - loss: 0.6941 - accuracy: 0.5233 - val_loss: 0.6933 - val_accuracy: 0.5075\n",
            "Epoch 19/20\n",
            "19/19 [==============================] - 101s 5s/step - loss: 0.6964 - accuracy: 0.4825 - val_loss: 0.6991 - val_accuracy: 0.5100\n",
            "Epoch 20/20\n",
            "19/19 [==============================] - 98s 5s/step - loss: 0.6996 - accuracy: 0.4842 - val_loss: 0.6950 - val_accuracy: 0.4900\n",
            "13/13 [==============================] - 7s 537ms/step\n",
            "Accuracy: 0.49\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.49      0.69      0.57       200\n",
            "    positive       0.48      0.29      0.37       200\n",
            "\n",
            "    accuracy                           0.49       400\n",
            "   macro avg       0.49      0.49      0.47       400\n",
            "weighted avg       0.49      0.49      0.47       400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8XfnnlZXyP7f"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uViGkPDPpP8"
      },
      "source": [
        "# Convolutional"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Crear el modelo CNN\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(word_index) + 1, 300, input_length=max_len))\n",
        "model.add(Conv1D(128, 5, activation=\"relu\"))\n",
        "model.add(MaxPooling1D(5))\n",
        "model.add(Conv1D(128, 5, activation=\"relu\"))\n",
        "model.add(MaxPooling1D(5))\n",
        "model.add(Conv1D(128, 5, activation=\"relu\"))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(2, activation='softmax'))  # Ajustar el número de unidades a 2\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(xtrain_pad, ytrain_one_hot, epochs=20, batch_size=64, validation_data=(xvalid_pad, yvalid_one_hot))\n",
        "\n",
        "# Predecir en el conjunto de validación\n",
        "preds = np.argmax(model.predict(xvalid_pad), axis=-1)\n",
        "\n",
        "# Calcular la precisión\n",
        "accuracy = metrics.accuracy_score(yvalid_encoded, preds)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Generar el reporte de clasificación\n",
        "target_names = label_encoder.classes_  # Esto debería ser ['negative', 'positive']\n",
        "clas_report = classification_report(yvalid_encoded, preds, target_names=target_names)\n",
        "print(clas_report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDhAosPp9NpT",
        "outputId": "f31eae06-86e8-4086-f9ba-7dd1be2140bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 2700, 300)         11829000  \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 2696, 128)         192128    \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1  (None, 539, 128)          0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 535, 128)          82048     \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPoolin  (None, 107, 128)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 103, 128)          82048     \n",
            "                                                                 \n",
            " global_max_pooling1d (Glob  (None, 128)               0         \n",
            " alMaxPooling1D)                                                 \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12201994 (46.55 MB)\n",
            "Trainable params: 12201994 (46.55 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "19/19 [==============================] - 144s 7s/step - loss: 0.7006 - accuracy: 0.5000 - val_loss: 0.6950 - val_accuracy: 0.5000\n",
            "Epoch 2/20\n",
            "19/19 [==============================] - 138s 7s/step - loss: 0.6952 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
            "Epoch 3/20\n",
            "19/19 [==============================] - 128s 7s/step - loss: 0.6919 - accuracy: 0.5142 - val_loss: 0.6929 - val_accuracy: 0.5250\n",
            "Epoch 4/20\n",
            "19/19 [==============================] - 118s 6s/step - loss: 0.6849 - accuracy: 0.6125 - val_loss: 0.6920 - val_accuracy: 0.5000\n",
            "Epoch 5/20\n",
            "19/19 [==============================] - 121s 6s/step - loss: 0.6458 - accuracy: 0.7342 - val_loss: 0.6707 - val_accuracy: 0.5100\n",
            "Epoch 6/20\n",
            "19/19 [==============================] - 129s 7s/step - loss: 0.4074 - accuracy: 0.8817 - val_loss: 0.5448 - val_accuracy: 0.7225\n",
            "Epoch 7/20\n",
            "19/19 [==============================] - 123s 7s/step - loss: 0.0467 - accuracy: 0.9925 - val_loss: 0.5697 - val_accuracy: 0.7875\n",
            "Epoch 8/20\n",
            "19/19 [==============================] - 120s 6s/step - loss: 0.0092 - accuracy: 0.9975 - val_loss: 0.8476 - val_accuracy: 0.7350\n",
            "Epoch 9/20\n",
            "19/19 [==============================] - 132s 7s/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.8171 - val_accuracy: 0.7700\n",
            "Epoch 10/20\n",
            "19/19 [==============================] - 130s 7s/step - loss: 5.9710e-04 - accuracy: 1.0000 - val_loss: 0.7309 - val_accuracy: 0.7900\n",
            "Epoch 11/20\n",
            "19/19 [==============================] - 130s 7s/step - loss: 3.2745e-04 - accuracy: 1.0000 - val_loss: 0.7632 - val_accuracy: 0.7925\n",
            "Epoch 12/20\n",
            "19/19 [==============================] - 119s 6s/step - loss: 2.2997e-04 - accuracy: 1.0000 - val_loss: 0.7851 - val_accuracy: 0.7875\n",
            "Epoch 13/20\n",
            "19/19 [==============================] - 129s 7s/step - loss: 5.1060e-04 - accuracy: 1.0000 - val_loss: 0.8053 - val_accuracy: 0.7775\n",
            "Epoch 14/20\n",
            "19/19 [==============================] - 119s 6s/step - loss: 1.6603e-04 - accuracy: 1.0000 - val_loss: 0.8294 - val_accuracy: 0.7750\n",
            "Epoch 15/20\n",
            "19/19 [==============================] - 128s 7s/step - loss: 1.9484e-04 - accuracy: 1.0000 - val_loss: 0.8569 - val_accuracy: 0.7725\n",
            "Epoch 16/20\n",
            "19/19 [==============================] - 128s 7s/step - loss: 1.0792e-04 - accuracy: 1.0000 - val_loss: 0.8716 - val_accuracy: 0.7750\n",
            "Epoch 17/20\n",
            "19/19 [==============================] - 119s 6s/step - loss: 6.7888e-05 - accuracy: 1.0000 - val_loss: 0.8701 - val_accuracy: 0.7750\n",
            "Epoch 18/20\n",
            "19/19 [==============================] - 130s 7s/step - loss: 5.9871e-05 - accuracy: 1.0000 - val_loss: 0.8787 - val_accuracy: 0.7750\n",
            "Epoch 19/20\n",
            "19/19 [==============================] - 119s 6s/step - loss: 1.7953e-04 - accuracy: 1.0000 - val_loss: 0.8754 - val_accuracy: 0.7775\n",
            "Epoch 20/20\n",
            "19/19 [==============================] - 129s 7s/step - loss: 8.9998e-05 - accuracy: 1.0000 - val_loss: 0.8722 - val_accuracy: 0.7800\n",
            "13/13 [==============================] - 10s 765ms/step\n",
            "Accuracy: 0.78\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.79      0.77      0.78       200\n",
            "    positive       0.77      0.79      0.78       200\n",
            "\n",
            "    accuracy                           0.78       400\n",
            "   macro avg       0.78      0.78      0.78       400\n",
            "weighted avg       0.78      0.78      0.78       400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O /content/drive/MyDrive/pretrainedwordvectorsforspanish/cc.es.300.vec.gz https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.es.300.vec.gz\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvptzE9BoBhv",
        "outputId": "251e9b5a-7da0-42c2-f5b2-a4f054673c0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-02 22:44:54--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.es.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 108.156.133.37, 108.156.133.4, 108.156.133.117, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|108.156.133.37|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1285580896 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘/content/drive/MyDrive/pretrainedwordvectorsforspanish/cc.es.300.vec.gz’\n",
            "\n",
            "/content/drive/MyDr 100%[===================>]   1.20G  23.7MB/s    in 53s     \n",
            "\n",
            "2024-07-02 22:45:48 (23.0 MB/s) - ‘/content/drive/MyDrive/pretrainedwordvectorsforspanish/cc.es.300.vec.gz’ saved [1285580896/1285580896]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gunzip /content/drive/MyDrive/pretrainedwordvectorsforspanish/cc.es.300.vec.gz\n"
      ],
      "metadata": {
        "id": "Y2OHy8Y-pfFE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "351c47a7-686b-421f-b644-d11044c5ac01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gzip: /content/drive/MyDrive/pretrainedwordvectorsforspanish/cc.es.300.vec already exists; do you wish to overwrite (y or n)? n\n",
            "\tnot overwritten\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "# Verificar si el archivo existe en la ruta especificada\n",
        "wordvectors_file_vec = '/content/drive/MyDrive/pretrainedwordvectorsforspanish/cc.es.300.vec'\n",
        "if os.path.exists(wordvectors_file_vec):\n",
        "    print(\"El archivo se ha encontrado correctamente.\")\n",
        "    # Cargar los embeddings preentrenados\n",
        "    wordvectors = KeyedVectors.load_word2vec_format(wordvectors_file_vec, binary=False)\n",
        "    print(\"Embeddings de FastText cargados exitosamente.\")\n",
        "else:\n",
        "    print(\"El archivo no se encuentra en la ruta especificada.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gp_kzGCpjHa",
        "outputId": "dc5a27a9-ddfc-4616-d99b-92dd4a85f85f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El archivo se ha encontrado correctamente.\n",
            "Embeddings de FastText cargados exitosamente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6tUeibXpQ1f"
      },
      "source": [
        "# **Word2Vec preentrenado**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1fM1AMtqhCLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear una matriz de embeddings donde cada fila representa un vector de una palabra\n",
        "embedding_dim = 300\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if word in wordvectors:\n",
        "        embedding_matrix[i] = wordvectors[word]\n",
        "    else:\n",
        "        embedding_matrix[i] = np.random.normal(0, np.sqrt(0.25), embedding_dim)\n",
        "\n",
        "# Crear un DataFrame para visualizar la matriz de embeddings\n",
        "# Seleccionar las primeras 10 filas para visualizar\n",
        "embedding_df = pd.DataFrame(embedding_matrix[:10], columns=[f\"dim_{i}\" for i in range(embedding_dim)])\n",
        "print(embedding_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-GXQK7tgo-j",
        "outputId": "e5c665a4-2d20-4de6-c810-168933067c1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    dim_0   dim_1   dim_2   dim_3   dim_4   dim_5   dim_6   dim_7   dim_8  \\\n",
            "0  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000   \n",
            "1  0.0844  0.1832  0.1464  0.0012  0.0093  0.0685  0.0003  0.0620  0.0818   \n",
            "2  0.0409 -0.0381  0.1203  0.0221  0.0250  0.0294  0.0568  0.0220 -0.0372   \n",
            "3 -0.0496  0.0836 -0.0782 -0.0708 -0.0174  0.0452 -0.0902  0.0514  0.0917   \n",
            "4 -0.0828  0.3689  0.2216 -0.1033  0.0435  0.1911  0.1409 -0.0305  0.1279   \n",
            "5 -0.3831  0.2311 -0.0661 -0.0249 -0.1404  0.1883  0.1389  0.1286  0.0913   \n",
            "6  0.0493  0.2054 -0.5362 -0.1148 -0.0362  0.1562  0.1540  0.0439  0.0193   \n",
            "7 -0.2347  0.2863  0.1340  0.0471  0.0360  0.4923 -0.3978  0.0184  0.0242   \n",
            "8  0.1588  0.0520  0.0253 -0.0447 -0.0017  0.0442  0.0296  0.0692  0.0442   \n",
            "9  0.2019 -0.0950 -0.1225 -0.0689  0.1043  0.3920 -0.2888  0.4975 -0.2205   \n",
            "\n",
            "    dim_9  ...  dim_290  dim_291  dim_292  dim_293  dim_294  dim_295  dim_296  \\\n",
            "0  0.0000  ...   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   \n",
            "1  0.0391  ...  -0.0685   0.1211   0.0877  -0.0230   0.0153   0.0589  -0.0479   \n",
            "2 -0.3381  ...  -0.0148   0.1489   0.0871   0.2638   0.0546  -0.0334   0.0724   \n",
            "3  0.1467  ...  -0.0348  -0.0910   0.0030   0.1737   0.0946   0.1046  -0.0780   \n",
            "4 -0.3683  ...   0.0900   0.3879   0.5362  -0.0073   0.1392  -0.0117  -0.2475   \n",
            "5  0.0198  ...  -0.0298   0.2997   0.3336  -0.0230  -0.1386  -0.1507   0.2677   \n",
            "6  0.0916  ...   0.5802   0.0204  -0.0082  -0.1647  -0.0337   0.0554  -0.3700   \n",
            "7 -0.1799  ...   0.2613   0.2623   0.3714   0.0394   0.0401   0.0151  -0.0153   \n",
            "8  0.0350  ...   0.0092   0.0865   0.0377   0.0228  -0.0333   0.0776  -0.1156   \n",
            "9  0.0254  ...  -0.2304   0.1590   0.2706   0.1159  -0.0472  -0.1745  -0.2453   \n",
            "\n",
            "   dim_297  dim_298  dim_299  \n",
            "0   0.0000   0.0000   0.0000  \n",
            "1  -0.0562   0.0268  -0.0127  \n",
            "2  -0.0296   0.0203   0.0508  \n",
            "3  -0.0277   0.0839   0.0556  \n",
            "4   0.0049   0.1163  -0.0115  \n",
            "5   0.0563   0.1286   0.0049  \n",
            "6  -0.1796   0.0191   0.1623  \n",
            "7  -0.0075  -0.0486   0.0408  \n",
            "8  -0.0248   0.0768  -0.0026  \n",
            "9  -0.0363   0.1778  -0.0348  \n",
            "\n",
            "[10 rows x 300 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def representar_review_como_vector(review):\n",
        "    palabras = review.split()\n",
        "    vectores = np.zeros(300)  # Suponiendo que los embeddings tienen 300 dimensiones\n",
        "    palabras_encontradas = 0\n",
        "    for palabra in palabras:\n",
        "        try:\n",
        "            vector = wordvectors.word_vec(palabra)\n",
        "            vectores = vectores + vector\n",
        "            palabras_encontradas = palabras_encontradas + 1\n",
        "        except KeyError:\n",
        "            print(\"No está la palabra \" + palabra)\n",
        "\n",
        "    if palabras_encontradas > 0:\n",
        "        promedio = vectores / palabras_encontradas\n",
        "    else:\n",
        "        promedio = vectores\n",
        "    return vectores, promedio\n",
        "\n"
      ],
      "metadata": {
        "id": "EW8ujoYnlmQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleccionar una review específica para probar\n",
        "review_ejemplo = train_df[train_df['review'].str.contains('the only problem')]['review'].values[0]\n",
        "\n",
        "# Obtener los vectores suma y promedio de la review\n",
        "suma, promedio = representar_review_como_vector(review_ejemplo)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlP7h51JoELd",
        "outputId": "c6344023-d004-4fb6-ccd7-81ae40ccc8a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No está la palabra linklater's\n",
            "No está la palabra $23\n",
            "No está la palabra immerses\n",
            "No está la palabra twentysomething\n",
            "No está la palabra college-town\n",
            "No está la palabra spouting\n",
            "No está la palabra inadequacy\n",
            "No está la palabra pervasiveness\n",
            "No está la palabra they've\n",
            "No está la palabra they'll\n",
            "No está la palabra linklater\n",
            "No está la palabra it's\n",
            "No está la palabra linklater\n",
            "No está la palabra meandering\n",
            "No está la palabra pseudo-intellectuals\n",
            "No está la palabra bribery-based\n",
            "No está la palabra scooby-doo\n",
            "No está la palabra unhinged\n",
            "No está la palabra conspiracies\n",
            "No está la palabra linklater\n",
            "No está la palabra what's\n",
            "No está la palabra aren't\n",
            "No está la palabra strangeness\n",
            "No está la palabra linklater\n",
            "No está la palabra oddballs\n",
            "No está la palabra well-deserved\n",
            "No está la palabra can't\n",
            "No está la palabra linklater\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-907522245eb5>:7: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
            "  vector = wordvectors.word_vec(palabra)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encontrar las palabras más similares al vector promedio\n",
        "print(\"Palabras similares al vector promedio:\")\n",
        "print(wordvectors.similar_by_vector(promedio, topn=10))\n",
        "\n",
        "# Encontrar las palabras más similares al vector suma\n",
        "print(\"Palabras similares al vector suma:\")\n",
        "print(wordvectors.similar_by_vector(suma, topn=10))\n",
        "\n",
        "# Normalizar el vector suma y encontrar palabras similares\n",
        "normalized_v = suma / np.linalg.norm(suma)\n",
        "print(\"Palabras similares al vector suma normalizado:\")\n",
        "print(wordvectors.similar_by_vector(normalized_v, topn=10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaMDhw-tp3-Q",
        "outputId": "f382a32f-ec0f-4fec-ffb4-6f76f371fbb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palabras similares al vector promedio:\n",
            "[('of', 0.7201033234596252), ('meI', 0.7176797389984131), ('the', 0.7163772583007812), ('that', 0.7091971039772034), ('to', 0.705650269985199), ('and', 0.7054423689842224), ('is', 0.6933733224868774), ('day.', 0.6874086260795593), ('it', 0.6863765716552734), ('whith', 0.6731469035148621)]\n",
            "Palabras similares al vector suma:\n",
            "[('of', 0.72010338306427), ('meI', 0.7176797986030579), ('the', 0.7163771986961365), ('that', 0.7091971039772034), ('to', 0.705650269985199), ('and', 0.7054423689842224), ('is', 0.6933733224868774), ('day.', 0.6874086260795593), ('it', 0.6863766312599182), ('whith', 0.6731469035148621)]\n",
            "Palabras similares al vector suma normalizado:\n",
            "[('of', 0.72010338306427), ('meI', 0.7176797389984131), ('the', 0.7163771986961365), ('that', 0.7091971039772034), ('to', 0.705650269985199), ('and', 0.7054423093795776), ('is', 0.6933733820915222), ('day.', 0.6874086260795593), ('it', 0.6863765716552734), ('whith', 0.6731469035148621)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIEZW-TTqs8V"
      },
      "source": [
        "## **Implementamos Doc2Vec**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPeCReMEo0IW",
        "outputId": "89ae4441-1475-4fc7-8349-0908915a6a40"
      },
      "source": [
        "\n",
        "problem = train_df[train_df['review'].str.contains('the only problem')]['review'].values[0]\n",
        "suma, promedio = representar_review_como_vector(problem)\n",
        "print(suma)\n",
        "print(wordvectors.similar_by_vector(promedio, topn=10))\n",
        "print(promedio)\n",
        "print(wordvectors.similar_by_vector(suma, topn=10))\n",
        "normalized_v = suma/np.linalg.norm(suma)\n",
        "print(wordvectors.similar_by_vector(normalized_v, topn=10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-907522245eb5>:7: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
            "  vector = wordvectors.word_vec(palabra)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No está la palabra linklater's\n",
            "No está la palabra $23\n",
            "No está la palabra immerses\n",
            "No está la palabra twentysomething\n",
            "No está la palabra college-town\n",
            "No está la palabra spouting\n",
            "No está la palabra inadequacy\n",
            "No está la palabra pervasiveness\n",
            "No está la palabra they've\n",
            "No está la palabra they'll\n",
            "No está la palabra linklater\n",
            "No está la palabra it's\n",
            "No está la palabra linklater\n",
            "No está la palabra meandering\n",
            "No está la palabra pseudo-intellectuals\n",
            "No está la palabra bribery-based\n",
            "No está la palabra scooby-doo\n",
            "No está la palabra unhinged\n",
            "No está la palabra conspiracies\n",
            "No está la palabra linklater\n",
            "No está la palabra what's\n",
            "No está la palabra aren't\n",
            "No está la palabra strangeness\n",
            "No está la palabra linklater\n",
            "No está la palabra oddballs\n",
            "No está la palabra well-deserved\n",
            "No está la palabra can't\n",
            "No está la palabra linklater\n",
            "[-7.20710010e+00  3.93489001e+01 -7.29539967e+00 -2.18482999e+01\n",
            " -3.30740005e+00  4.61368001e+01 -8.18100052e-01  1.17095001e+01\n",
            "  9.05850008e+00  7.53830050e+00 -2.62093998e+01  5.74289005e+01\n",
            "  5.82419978e+00  3.02980031e+00 -2.56751999e+01  1.94208002e+01\n",
            "  2.57880006e+00  4.58460010e+01 -1.33979986e+00 -2.97666001e+01\n",
            " -8.17789947e+00  1.41884000e+01 -1.41257999e+01 -8.53709948e+00\n",
            "  5.17534995e+01 -4.80949995e+01  2.66968001e+01  4.25050026e+00\n",
            "  7.46123001e+01 -2.93040005e+00  1.42481002e+01  3.22480002e+01\n",
            " -1.34716999e+01 -1.35027000e+01  2.53132000e+01  1.82322999e+01\n",
            " -5.39150999e+01 -1.60785003e+01 -9.19990008e+00 -6.53759993e+00\n",
            "  1.09052499e+02  5.40987992e+01  6.83429991e+00  2.12429000e+01\n",
            " -2.58056003e+01 -2.28515000e+01 -8.98659898e+00  9.89609983e+00\n",
            " -2.34500207e-01  3.46488001e+01 -2.78770996e+01 -6.11702001e+01\n",
            " -1.31470018e+00  1.84691001e+01 -2.59622998e+01 -3.88615001e+01\n",
            " -2.23200996e+01  1.45354001e+01  4.56870997e+01  2.74387004e+01\n",
            " -9.52820007e+00 -1.74277002e+01  3.33312998e+01 -3.42555000e+01\n",
            "  1.84171002e+01  9.82759963e+00 -2.27055005e+01  1.04082901e+02\n",
            " -1.13690008e+00  5.64009968e+00  9.11849003e+01 -4.02876004e+01\n",
            "  3.43102003e+01 -3.40222996e+01 -2.40522000e+01 -2.15592002e+01\n",
            " -6.19677996e+01 -2.33543000e+01 -2.23661999e+01  2.58780003e+01\n",
            "  5.45051999e+01 -6.15550022e+00 -1.06459993e+00 -2.72740005e+00\n",
            " -4.67178000e+01 -1.36257999e+01 -2.60741001e+01  2.19599986e+00\n",
            " -4.29059999e+01 -2.31042998e+01 -1.80324001e+01  1.37829000e+01\n",
            " -8.35319982e+00  8.22959991e+00  4.89330002e+01  2.97002000e+01\n",
            " -5.53746002e+01  1.33265000e+01 -6.17680993e+01  2.66624996e+01\n",
            " -1.85463996e+01  4.47322999e+01  6.22710003e+01 -1.86045001e+01\n",
            "  1.98740000e+01  1.14310300e+02 -2.79387999e+01  3.21048002e+01\n",
            "  7.38974003e+01 -1.59520004e+01 -1.24367999e+01 -2.42436002e+01\n",
            "  1.07018002e+01 -2.36783999e+01 -5.77313009e+01  6.59899804e-01\n",
            " -1.57230999e+01 -3.53677002e+01  4.01804002e+01 -8.53799587e-01\n",
            " -4.84174001e+01 -3.09934998e+01 -5.15112002e+01 -9.23060003e+00\n",
            "  2.21541997e+01  3.52960999e+01 -2.45376001e+01 -4.75369981e+00\n",
            "  5.04192997e+01  6.10649996e+00 -2.95199920e-01  7.54613006e+01\n",
            " -2.24661000e+01 -3.73501005e+01  3.73959987e+00 -3.77045001e+01\n",
            "  3.18632998e+01 -1.23964001e+01 -3.80442006e+01 -8.33880006e+00\n",
            "  3.07570001e+01  8.65432998e+01  5.63890026e+00 -1.38670997e+01\n",
            " -1.51760998e+01 -4.02109987e+00  1.04709979e+00 -6.49668997e+01\n",
            " -2.38839999e+01 -7.82440001e+00  1.07988000e+01 -6.69049001e+01\n",
            " -2.44219999e+01 -2.72956999e+01  1.83545994e+01 -6.53369976e+00\n",
            " -2.37840036e+00  4.49929002e+01 -7.93970014e+00 -3.92240004e+00\n",
            " -6.58909989e+00  1.76444998e+01  2.49847999e+01 -2.74787002e+01\n",
            "  1.02903999e+01  1.20113999e+01 -1.61329999e+01 -2.18452999e+01\n",
            "  2.47289997e+01  5.02800015e+00 -3.37100005e+00  1.92457990e+01\n",
            " -1.09110000e+01 -5.34639998e+00 -2.06282997e+01  4.83989996e+01\n",
            "  3.95329003e+01 -2.91060022e+00  2.11647996e+01  1.80921998e+01\n",
            "  4.97391990e+01 -1.88428996e+01 -2.11922999e+01 -2.93210003e+01\n",
            "  3.88740014e+00  2.36419998e+01 -2.31490997e+01  1.34573001e+01\n",
            " -6.88621003e+01 -1.96310998e+01  1.46995002e+01 -1.15621002e+01\n",
            " -3.54718999e+01 -1.18722002e+01 -3.41442000e+01  2.89584000e+01\n",
            " -4.97494001e+01  3.19934998e+01  7.25789958e+00  4.10779903e+00\n",
            "  3.39406000e+01 -1.56759699e+02  4.35045002e+01 -2.36338002e+01\n",
            " -4.69591992e+01 -5.45699997e+01  1.67929972e+00 -4.14871303e+02\n",
            " -1.80485002e+01 -1.07066999e+01 -6.23109002e+01  1.40280000e+01\n",
            "  1.71991009e+01  2.78593000e+01  3.64309002e+01 -1.35128000e+01\n",
            " -3.53972004e+01 -1.96013999e+01 -3.08159002e+01 -2.70323000e+01\n",
            " -3.22699982e+00  3.60993999e+01 -3.84229001e+01  3.47345999e+01\n",
            "  6.78050029e+00  5.97739997e+00 -3.19164998e+01  5.59630021e+00\n",
            "  1.57913999e+01 -4.54250007e+00 -1.41565000e+01 -1.79426602e+02\n",
            " -2.87676000e+01 -2.23379988e+00 -1.84765000e+01  1.51972000e+01\n",
            "  1.21145000e+01 -8.49939984e+00 -2.41198998e+01  5.36435006e+01\n",
            " -3.76304002e+01 -1.60566000e+01 -1.95753002e+01 -5.01631001e+01\n",
            " -4.87470012e+00 -2.36289000e+01 -2.01430001e+00 -5.50651002e+01\n",
            "  2.21241999e+01 -2.69917998e+01  5.85989000e+01 -5.61599992e+00\n",
            "  1.07588699e+02 -4.32180005e+00  1.54188000e+01  1.23101700e+02\n",
            " -7.07093006e+01  3.00834999e+01  3.12213001e+01  1.92799000e+01\n",
            "  8.91930003e+00 -6.76320015e+00  2.23371001e+01  3.07566001e+01\n",
            " -2.85011004e+01  9.48549992e+00 -7.23099995e+00 -6.39989987e+00\n",
            "  1.57646003e+01 -3.11783001e+01  1.08039001e+01 -1.62738000e+01\n",
            "  1.34771003e+01  1.74470002e+01  1.37568999e+01 -1.05969801e+02\n",
            " -1.18105001e+01  8.31240025e+00 -1.02761001e+01 -3.26135997e+01\n",
            " -1.57549000e+01 -1.94332002e+01  7.68099977e-01 -3.13953999e+01\n",
            "  3.40883001e+01  1.24574004e+01 -1.46770998e+01  6.77699992e+00\n",
            "  5.19028999e+01 -5.68283998e+01 -1.79869979e+00  4.31539998e+00\n",
            "  4.23933998e+01  2.34101996e+01  8.03560008e+00  8.88939993e+00\n",
            " -2.58092002e+01 -1.28895001e+01  2.97313000e+01  6.03999972e-01]\n",
            "[('of', 0.7201033234596252), ('meI', 0.7176797389984131), ('the', 0.7163772583007812), ('that', 0.7091971039772034), ('to', 0.705650269985199), ('and', 0.7054423689842224), ('is', 0.6933733224868774), ('day.', 0.6874086260795593), ('it', 0.6863765716552734), ('whith', 0.6731469035148621)]\n",
            "[-1.06930269e-02  5.83811574e-02 -1.08240351e-02 -3.24158753e-02\n",
            " -4.90712173e-03  6.84522257e-02 -1.21379830e-03  1.73731455e-02\n",
            "  1.34399111e-02  1.11844221e-02 -3.88863499e-02  8.52060839e-02\n",
            "  8.64124596e-03  4.49525268e-03 -3.80937684e-02  2.88142436e-02\n",
            "  3.82611285e-03  6.80207729e-02 -1.98783362e-03 -4.41640951e-02\n",
            " -1.21333820e-02  2.10510386e-02 -2.09581600e-02 -1.26663197e-02\n",
            "  7.67856075e-02 -7.13575661e-02  3.96094957e-02  6.30638020e-03\n",
            "  1.10700742e-01 -4.34777456e-03  2.11396145e-02  4.78456976e-02\n",
            " -1.99876853e-02 -2.00336795e-02  3.75566765e-02  2.70508900e-02\n",
            " -7.99927298e-02 -2.38553417e-02 -1.36497034e-02 -9.69970317e-03\n",
            "  1.61798960e-01  8.02652807e-02  1.01399109e-02  3.15176557e-02\n",
            " -3.82872408e-02 -3.39043026e-02 -1.33332329e-02  1.46826407e-02\n",
            " -3.47923155e-04  5.14077153e-02 -4.13606819e-02 -9.07569735e-02\n",
            " -1.95059374e-03  2.74022256e-02 -3.85197326e-02 -5.76580121e-02\n",
            " -3.31158748e-02  2.15658755e-02  6.77850143e-02  4.07102380e-02\n",
            " -1.41367954e-02 -2.58571219e-02  4.94529671e-02 -5.08241840e-02\n",
            "  2.73250745e-02  1.45810084e-02 -3.36876862e-02  1.54425669e-01\n",
            " -1.68679537e-03  8.36810041e-03  1.35289170e-01 -5.97738878e-02\n",
            "  5.09053417e-02 -5.04781894e-02 -3.56857567e-02 -3.19869439e-02\n",
            " -9.19403555e-02 -3.46502967e-02 -3.31842728e-02  3.83946592e-02\n",
            "  8.08682491e-02 -9.13278965e-03 -1.57952512e-03 -4.04658761e-03\n",
            " -6.93142433e-02 -2.02163204e-02 -3.86856084e-02  3.25816003e-03\n",
            " -6.36587536e-02 -3.42793766e-02 -2.67543028e-02  2.04494065e-02\n",
            " -1.23934715e-02  1.22100889e-02  7.26008906e-02  4.40655786e-02\n",
            " -8.21581605e-02  1.97722552e-02 -9.16440643e-02  3.95586047e-02\n",
            " -2.75169133e-02  6.63683975e-02  9.23902082e-02 -2.76031158e-02\n",
            "  2.94866469e-02  1.69599852e-01 -4.14522254e-02  4.76332348e-02\n",
            "  1.09640060e-01 -2.36676564e-02 -1.84522254e-02 -3.59697333e-02\n",
            "  1.58780419e-02 -3.51311572e-02 -8.56547492e-02  9.79079828e-04\n",
            " -2.33280413e-02 -5.24743326e-02  5.96148370e-02 -1.26676497e-03\n",
            " -7.18359052e-02 -4.59844211e-02 -7.64261130e-02 -1.36952523e-02\n",
            "  3.28697325e-02  5.23681007e-02 -3.64059349e-02 -7.05296708e-03\n",
            "  7.48060826e-02  9.06008897e-03 -4.37982078e-04  1.11960387e-01\n",
            " -3.33324925e-02 -5.54155794e-02  5.54836777e-03 -5.59413947e-02\n",
            "  4.72749255e-02 -1.83922850e-02 -5.64454015e-02 -1.23721069e-02\n",
            "  4.56335313e-02  1.28402522e-01  8.36632086e-03 -2.05743319e-02\n",
            " -2.25164685e-02 -5.96602354e-03  1.55356052e-03 -9.63900589e-02\n",
            " -3.54362017e-02 -1.16089021e-02  1.60219585e-02 -9.92654304e-02\n",
            " -3.62344211e-02 -4.04980711e-02  2.72323434e-02 -9.69391656e-03\n",
            " -3.52878392e-03  6.67550448e-02 -1.17799705e-02 -5.81958463e-03\n",
            " -9.77611260e-03  2.61787832e-02  3.70694361e-02 -4.07695849e-02\n",
            "  1.52676556e-02  1.78210681e-02 -2.39362017e-02 -3.24114241e-02\n",
            "  3.66899105e-02  7.45994087e-03 -5.00148375e-03  2.85545980e-02\n",
            " -1.61884273e-02 -7.93234419e-03 -3.06057859e-02  7.18086048e-02\n",
            "  5.86541548e-02 -4.31839796e-03  3.14017799e-02  2.68430264e-02\n",
            "  7.37970312e-02 -2.79568243e-02 -3.14425814e-02 -4.35029678e-02\n",
            "  5.76765599e-03  3.50771511e-02 -3.43458452e-02  1.99663206e-02\n",
            " -1.02169288e-01 -2.91262608e-02  2.18093475e-02 -1.71544513e-02\n",
            " -5.26289316e-02 -1.76145404e-02 -5.06590504e-02  4.29649852e-02\n",
            " -7.38121664e-02  4.74681006e-02  1.07683970e-02  6.09465732e-03\n",
            "  5.03569733e-02 -2.32581156e-01  6.45467363e-02 -3.50649854e-02\n",
            " -6.96724024e-02 -8.09643912e-02  2.49154261e-03 -6.15536057e-01\n",
            " -2.67781902e-02 -1.58853114e-02 -9.24494068e-02  2.08130564e-02\n",
            "  2.55179538e-02  4.13342730e-02  5.40517807e-02 -2.00486646e-02\n",
            " -5.25181014e-02 -2.90821957e-02 -4.57209201e-02 -4.01072700e-02\n",
            " -4.78783356e-03  5.35599405e-02 -5.70072701e-02  5.15350147e-02\n",
            "  1.00600895e-02  8.86854595e-03 -4.73538573e-02  8.30311604e-03\n",
            "  2.34293767e-02 -6.73961435e-03 -2.10037092e-02 -2.66211575e-01\n",
            " -4.26818992e-02 -3.31424314e-03 -2.74132047e-02  2.25477745e-02\n",
            "  1.79740356e-02 -1.26103855e-02 -3.57862015e-02  7.95897635e-02\n",
            " -5.58314543e-02 -2.38228486e-02 -2.90434721e-02 -7.44259645e-02\n",
            " -7.23249276e-03 -3.50577152e-02 -2.98857568e-03 -8.16989617e-02\n",
            "  3.28252224e-02 -4.00471806e-02  8.69419881e-02 -8.33234410e-03\n",
            "  1.59627150e-01 -6.41216625e-03  2.28765579e-02  1.82643472e-01\n",
            " -1.04909942e-01  4.46342728e-02  4.63224037e-02  2.86051929e-02\n",
            "  1.32333828e-02 -1.00344216e-02  3.31410980e-02  4.56329379e-02\n",
            " -4.22864991e-02  1.40734420e-02 -1.07284866e-02 -9.49540040e-03\n",
            "  2.33896147e-02 -4.62586054e-02  1.60295254e-02 -2.41451039e-02\n",
            "  1.99956978e-02  2.58857569e-02  2.04108307e-02 -1.57225224e-01\n",
            " -1.75229971e-02  1.23329381e-02 -1.52464393e-02 -4.83881301e-02\n",
            " -2.33752226e-02 -2.88326413e-02  1.13961421e-03 -4.65807121e-02\n",
            "  5.05761129e-02  1.84827899e-02 -2.17761125e-02  1.00548960e-02\n",
            "  7.70072699e-02 -8.43151332e-02 -2.66869405e-03  6.40267059e-03\n",
            "  6.28982193e-02  3.47332339e-02  1.19222553e-02  1.31890207e-02\n",
            " -3.82925819e-02 -1.91238874e-02  4.41117210e-02  8.96142392e-04]\n",
            "[('of', 0.72010338306427), ('meI', 0.7176797986030579), ('the', 0.7163771986961365), ('that', 0.7091971039772034), ('to', 0.705650269985199), ('and', 0.7054423689842224), ('is', 0.6933733224868774), ('day.', 0.6874086260795593), ('it', 0.6863766312599182), ('whith', 0.6731469035148621)]\n",
            "[('of', 0.72010338306427), ('meI', 0.7176797389984131), ('the', 0.7163771986961365), ('that', 0.7091971039772034), ('to', 0.705650269985199), ('and', 0.7054423093795776), ('is', 0.6933733820915222), ('day.', 0.6874086260795593), ('it', 0.6863765716552734), ('whith', 0.6731469035148621)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def representar_review_como_vector_promedio(review):\n",
        "    # Utiliza la función previamente definida para obtener los vectores suma y promedio\n",
        "    vectores, promedio = representar_review_como_vector(review)\n",
        "    return promedio\n"
      ],
      "metadata": {
        "id": "rZPUAlOgqOT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Buscar reviews que contienen la frase 'the only problem'\n",
        "matching_reviews = train_df[train_df['review'].str.contains('the only problem', case=False)]\n",
        "\n",
        "# Verificar si se encontraron reviews\n",
        "if matching_reviews.empty:\n",
        "    print(\"No se encontraron reviews que contengan la frase 'the only problem'.\")\n",
        "else:\n",
        "    # Seleccionar la primera review que coincide\n",
        "    review_ejemplo = matching_reviews['review'].values[0]\n",
        "\n",
        "    # Obtener el vector promedio de la review\n",
        "    promedio = representar_review_como_vector_promedio(review_ejemplo)\n",
        "\n",
        "    # Encontrar las palabras más similares al vector promedio\n",
        "    print(\"Palabras similares al vector promedio:\")\n",
        "    print(wordvectors.similar_by_vector(promedio, topn=10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rII0xuQPqS1O",
        "outputId": "98410791-e88c-4eee-e37c-6fe0d629ad62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-907522245eb5>:7: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
            "  vector = wordvectors.word_vec(palabra)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No está la palabra linklater's\n",
            "No está la palabra $23\n",
            "No está la palabra immerses\n",
            "No está la palabra twentysomething\n",
            "No está la palabra college-town\n",
            "No está la palabra spouting\n",
            "No está la palabra inadequacy\n",
            "No está la palabra pervasiveness\n",
            "No está la palabra they've\n",
            "No está la palabra they'll\n",
            "No está la palabra linklater\n",
            "No está la palabra it's\n",
            "No está la palabra linklater\n",
            "No está la palabra meandering\n",
            "No está la palabra pseudo-intellectuals\n",
            "No está la palabra bribery-based\n",
            "No está la palabra scooby-doo\n",
            "No está la palabra unhinged\n",
            "No está la palabra conspiracies\n",
            "No está la palabra linklater\n",
            "No está la palabra what's\n",
            "No está la palabra aren't\n",
            "No está la palabra strangeness\n",
            "No está la palabra linklater\n",
            "No está la palabra oddballs\n",
            "No está la palabra well-deserved\n",
            "No está la palabra can't\n",
            "No está la palabra linklater\n",
            "Palabras similares al vector promedio:\n",
            "[('of', 0.7201033234596252), ('meI', 0.7176797389984131), ('the', 0.7163772583007812), ('that', 0.7091971039772034), ('to', 0.705650269985199), ('and', 0.7054423689842224), ('is', 0.6933733224868774), ('day.', 0.6874086260795593), ('it', 0.6863765716552734), ('whith', 0.6731469035148621)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Drco-DEoOb6w"
      },
      "source": [
        "# Red RNN  pero con Embeddings preentrenado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReOXN3bAOXMm",
        "outputId": "8cc13773-b960-4176-8ac0-bd0613ab1b69"
      },
      "source": [
        "%%time\n",
        "with strategy.scope():\n",
        "    # A simpleRNN without any pretrained embeddings and one dense layer\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(word_index) + 1,\n",
        "                     300,\n",
        "                     weights=[embedding_matrix],\n",
        "                     input_length=max_len,\n",
        "                     trainable=False))\n",
        "    model.add(Conv1D(128, 5, activation=\"relu\"))\n",
        "    model.add(MaxPooling1D(5))\n",
        "    model.add(Conv1D(128, 5, activation=\"relu\"))\n",
        "    model.add(MaxPooling1D(5))\n",
        "    model.add(Conv1D(128, 5, activation=\"relu\"))\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(2, activation='sigmoid'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(xtrain_pad, ytrain_one_hot, epochs=20, batch_size=64*strategy.num_replicas_in_sync, validation_data=(xvalid_pad,yvalid_one_hot))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "19/19 [==============================] - 11s 163ms/step - loss: 0.7375 - accuracy: 0.4933 - val_loss: 0.6952 - val_accuracy: 0.4975\n",
            "Epoch 2/20\n",
            "19/19 [==============================] - 1s 61ms/step - loss: 0.6919 - accuracy: 0.5175 - val_loss: 0.6921 - val_accuracy: 0.5175\n",
            "Epoch 3/20\n",
            "19/19 [==============================] - 1s 61ms/step - loss: 0.6821 - accuracy: 0.5600 - val_loss: 0.6894 - val_accuracy: 0.5400\n",
            "Epoch 4/20\n",
            "19/19 [==============================] - 1s 61ms/step - loss: 0.6391 - accuracy: 0.6842 - val_loss: 0.6803 - val_accuracy: 0.5700\n",
            "Epoch 5/20\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.5998 - accuracy: 0.6900 - val_loss: 0.7123 - val_accuracy: 0.5175\n",
            "Epoch 6/20\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.4688 - accuracy: 0.8183 - val_loss: 0.6489 - val_accuracy: 0.5950\n",
            "Epoch 7/20\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.2634 - accuracy: 0.9250 - val_loss: 0.7150 - val_accuracy: 0.6100\n",
            "Epoch 8/20\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.1125 - accuracy: 0.9733 - val_loss: 0.7273 - val_accuracy: 0.6075\n",
            "Epoch 9/20\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0378 - accuracy: 0.9958 - val_loss: 1.0250 - val_accuracy: 0.6175\n",
            "Epoch 10/20\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0162 - accuracy: 0.9983 - val_loss: 0.8539 - val_accuracy: 0.6325\n",
            "Epoch 11/20\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.9704 - val_accuracy: 0.6350\n",
            "Epoch 12/20\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.9512 - val_accuracy: 0.6275\n",
            "Epoch 13/20\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.9837 - val_accuracy: 0.6225\n",
            "Epoch 14/20\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 1.0051 - val_accuracy: 0.6225\n",
            "Epoch 15/20\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 1.0588 - val_accuracy: 0.6425\n",
            "Epoch 16/20\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.0696 - val_accuracy: 0.6425\n",
            "Epoch 17/20\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 1.0912 - val_accuracy: 0.6350\n",
            "Epoch 18/20\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 8.5842e-04 - accuracy: 1.0000 - val_loss: 1.0829 - val_accuracy: 0.6350\n",
            "Epoch 19/20\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 7.0644e-04 - accuracy: 1.0000 - val_loss: 1.1029 - val_accuracy: 0.6325\n",
            "Epoch 20/20\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 4.5243e-04 - accuracy: 1.0000 - val_loss: 1.1065 - val_accuracy: 0.6450\n",
            "CPU times: user 21.6 s, sys: 1.36 s, total: 22.9 s\n",
            "Wall time: 35 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c50e74b18a0>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predecir en el conjunto de validación\n",
        "preds = np.argmax(model.predict(xvalid_pad), axis=-1)\n",
        "\n",
        "# Calcular la precisión\n",
        "accuracy = metrics.accuracy_score(yvalid_encoded, preds)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Generar el reporte de clasificación\n",
        "target_names = label_encoder.classes_  # Esto debería ser ['negative', 'positive']\n",
        "clas_report = classification_report(yvalid_encoded, preds, target_names=target_names)\n",
        "print(clas_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kk8NlGCDmQer",
        "outputId": "3bdd00d3-672e-4e33-8b3d-5dbcb3ef62e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 1s 11ms/step\n",
            "Accuracy: 0.645\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.66      0.60      0.63       200\n",
            "    positive       0.63      0.69      0.66       200\n",
            "\n",
            "    accuracy                           0.65       400\n",
            "   macro avg       0.65      0.65      0.64       400\n",
            "weighted avg       0.65      0.65      0.64       400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQbY143FqUwf"
      },
      "source": [
        "# **LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el modelo\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "with strategy.scope():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=len(word_index) + 1,\n",
        "                        output_dim=embedding_dim,\n",
        "                        weights=[embedding_matrix],\n",
        "                        input_length=max_len,\n",
        "                        trainable=False))\n",
        "    model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
        "    model.add(Dense(2, activation='sigmoid'))  # Cambiado a 2 para clasificación binaria\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZ-6UF3ktyBr",
        "outputId": "907a2029-374d-4de6-e8ea-e0b94cd3b381"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 2700, 300)         11787000  \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 100)               160400    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 2)                 202       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11947602 (45.58 MB)\n",
            "Trainable params: 160602 (627.35 KB)\n",
            "Non-trainable params: 11787000 (44.96 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Zr546G0tsY4",
        "outputId": "74d9f171-776b-4155-8d86-bf8ddf8dc990"
      },
      "source": [
        "model.fit(xtrain_pad, ytrain_one_hot, epochs=20, batch_size=64*strategy.num_replicas_in_sync, validation_data=(xvalid_pad,yvalid_one_hot))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "19/19 [==============================] - 237s 12s/step - loss: 0.6932 - accuracy: 0.4900 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 2/20\n",
            "19/19 [==============================] - 205s 11s/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 3/20\n",
            "19/19 [==============================] - 207s 11s/step - loss: 0.6932 - accuracy: 0.4983 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 4/20\n",
            "19/19 [==============================] - 208s 11s/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 5/20\n",
            "19/19 [==============================] - 204s 11s/step - loss: 0.6932 - accuracy: 0.4792 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 6/20\n",
            "19/19 [==============================] - 208s 11s/step - loss: 0.6932 - accuracy: 0.4642 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 7/20\n",
            "19/19 [==============================] - 207s 11s/step - loss: 0.6933 - accuracy: 0.4800 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 8/20\n",
            "19/19 [==============================] - 205s 11s/step - loss: 0.6932 - accuracy: 0.4975 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 9/20\n",
            "19/19 [==============================] - 204s 11s/step - loss: 0.6932 - accuracy: 0.4758 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 10/20\n",
            "19/19 [==============================] - 205s 11s/step - loss: 0.6932 - accuracy: 0.4933 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 11/20\n",
            "19/19 [==============================] - 206s 11s/step - loss: 0.6932 - accuracy: 0.4908 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 12/20\n",
            "19/19 [==============================] - 203s 11s/step - loss: 0.6932 - accuracy: 0.4775 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 13/20\n",
            "19/19 [==============================] - 203s 11s/step - loss: 0.6932 - accuracy: 0.4725 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 14/20\n",
            "19/19 [==============================] - 204s 11s/step - loss: 0.6932 - accuracy: 0.4867 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 15/20\n",
            "19/19 [==============================] - 204s 11s/step - loss: 0.6932 - accuracy: 0.4867 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 16/20\n",
            "19/19 [==============================] - 204s 11s/step - loss: 0.6932 - accuracy: 0.4892 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 17/20\n",
            "19/19 [==============================] - 205s 11s/step - loss: 0.6932 - accuracy: 0.5050 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 18/20\n",
            "19/19 [==============================] - 225s 12s/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 19/20\n",
            "19/19 [==============================] - 209s 11s/step - loss: 0.6932 - accuracy: 0.4858 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 20/20\n",
            "19/19 [==============================] - 205s 11s/step - loss: 0.6933 - accuracy: 0.4858 - val_loss: 0.6931 - val_accuracy: 0.5000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c50e0221750>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predecir en el conjunto de validación\n",
        "preds = np.argmax(model.predict(xvalid_pad), axis=-1)\n",
        "\n",
        "# Calcular la precisión\n",
        "accuracy = metrics.accuracy_score(yvalid_encoded, preds)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Generar el reporte de clasificación\n",
        "target_names = label_encoder.classes_  # Esto debería ser ['negative', 'positive']\n",
        "clas_report = classification_report(yvalid_encoded, preds, target_names=target_names)\n",
        "print(clas_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qi9BlwIm8MRp",
        "outputId": "063bc502-0ebf-47f3-b93f-2933e7953730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 11s 797ms/step\n",
            "Accuracy: 0.5\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.50      1.00      0.67       200\n",
            "    positive       0.00      0.00      0.00       200\n",
            "\n",
            "    accuracy                           0.50       400\n",
            "   macro avg       0.25      0.50      0.33       400\n",
            "weighted avg       0.25      0.50      0.33       400\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXwtcBAXqZec"
      },
      "source": [
        "# **GRU**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "with strategy.scope():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=len(word_index) + 1,\n",
        "                        output_dim=embedding_dim,\n",
        "                        weights=[embedding_matrix],\n",
        "                        input_length=max_len,\n",
        "                        trainable=False))\n",
        "    model.add(SpatialDropout1D(0.3))\n",
        "    model.add(GRU(300))\n",
        "    model.add(Dense(2, activation='sigmoid'))  # Cambiado a 2 para clasificación binaria\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpYtKYhnsk0L",
        "outputId": "a3f0bc08-06be-4af7-dad5-7cbecc34ff11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 2700, 300)         11787000  \n",
            "                                                                 \n",
            " spatial_dropout1d_1 (Spati  (None, 2700, 300)         0         \n",
            " alDropout1D)                                                    \n",
            "                                                                 \n",
            " gru_1 (GRU)                 (None, 300)               541800    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 602       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12329402 (47.03 MB)\n",
            "Trainable params: 542402 (2.07 MB)\n",
            "Non-trainable params: 11787000 (44.96 MB)\n",
            "_________________________________________________________________\n",
            "CPU times: user 323 ms, sys: 54.8 ms, total: 378 ms\n",
            "Wall time: 372 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ncf-UiS2Lxk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2833c6a-1801-4a3c-bed3-ebe51c80b507"
      },
      "source": [
        "model.fit(xtrain_pad, ytrain_one_hot, epochs=20, batch_size=64*strategy.num_replicas_in_sync, validation_data=(xvalid_pad,yvalid_one_hot))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "19/19 [==============================] - 12s 373ms/step - loss: 0.6941 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 2/20\n",
            "19/19 [==============================] - 6s 308ms/step - loss: 0.6933 - accuracy: 0.5033 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
            "Epoch 3/20\n",
            "19/19 [==============================] - 6s 309ms/step - loss: 0.6939 - accuracy: 0.4567 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 4/20\n",
            "19/19 [==============================] - 6s 344ms/step - loss: 0.6932 - accuracy: 0.5067 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 5/20\n",
            "19/19 [==============================] - 6s 317ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 6/20\n",
            "19/19 [==============================] - 6s 316ms/step - loss: 0.6932 - accuracy: 0.4800 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 7/20\n",
            "19/19 [==============================] - 7s 351ms/step - loss: 0.6932 - accuracy: 0.4933 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 8/20\n",
            "19/19 [==============================] - 6s 322ms/step - loss: 0.6932 - accuracy: 0.4817 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 9/20\n",
            "19/19 [==============================] - 7s 354ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 10/20\n",
            "19/19 [==============================] - 6s 317ms/step - loss: 0.6932 - accuracy: 0.4850 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 11/20\n",
            "19/19 [==============================] - 6s 318ms/step - loss: 0.6932 - accuracy: 0.4867 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 12/20\n",
            "19/19 [==============================] - 6s 315ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 13/20\n",
            "19/19 [==============================] - 6s 314ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 14/20\n",
            "19/19 [==============================] - 6s 344ms/step - loss: 0.6932 - accuracy: 0.4733 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 15/20\n",
            "19/19 [==============================] - 6s 311ms/step - loss: 0.6933 - accuracy: 0.4850 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 16/20\n",
            "19/19 [==============================] - 6s 313ms/step - loss: 0.6932 - accuracy: 0.4917 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 17/20\n",
            "19/19 [==============================] - 6s 344ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 18/20\n",
            "19/19 [==============================] - 6s 314ms/step - loss: 0.6932 - accuracy: 0.4817 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 19/20\n",
            "19/19 [==============================] - 6s 346ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 20/20\n",
            "19/19 [==============================] - 6s 316ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c29406f0f70>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predecir en el conjunto de validación\n",
        "preds = np.argmax(model.predict(xvalid_pad), axis=-1)\n",
        "\n",
        "# Calcular la precisión\n",
        "accuracy = metrics.accuracy_score(yvalid_encoded, preds)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Generar el reporte de clasificación\n",
        "target_names = label_encoder.classes_  # Esto debería ser ['negative', 'positive']\n",
        "clas_report = classification_report(yvalid_encoded, preds, target_names=target_names)\n",
        "print(clas_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5r-6LcmIuapP",
        "outputId": "f1b2e858-c41c-4943-c490-70669c4efe0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 1s 71ms/step\n",
            "Accuracy: 0.5\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.50      1.00      0.67       200\n",
            "    positive       0.00      0.00      0.00       200\n",
            "\n",
            "    accuracy                           0.50       400\n",
            "   macro avg       0.25      0.50      0.33       400\n",
            "weighted avg       0.25      0.50      0.33       400\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dByA5hFqqfO9"
      },
      "source": [
        "# **bidirectional LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Definir el modelo bidireccional LSTM\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "\n",
        "with strategy.scope():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=len(word_index) + 1,\n",
        "                        output_dim=embedding_dim,\n",
        "                        weights=[embedding_matrix],\n",
        "                        input_length=max_len,\n",
        "                        trainable=False))\n",
        "    model.add(Bidirectional(LSTM(128, dropout=0.3, recurrent_dropout=0.3)))\n",
        "    model.add(Dense(2, activation='sigmoid'))  # Cambiado a 2 para clasificación binaria\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWHoxIFPv6gR",
        "outputId": "62f48756-5ff6-4cac-dc3c-32aa6d7c18b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 2700, 300)         11883000  \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 256)               439296    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 514       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12322810 (47.01 MB)\n",
            "Trainable params: 439810 (1.68 MB)\n",
            "Non-trainable params: 11883000 (45.33 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el callback personalizado\n",
        "import datetime\n",
        "class TimeHistory(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        self.epoch_time_start = datetime.datetime.now()\n",
        "        print(f\"Epoch {epoch + 1} start time: {self.epoch_time_start}\")\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.epoch_time_end = datetime.datetime.now()\n",
        "        print(f\"Epoch {epoch + 1} end time: {self.epoch_time_end}\")\n"
      ],
      "metadata": {
        "id": "_8xI7dB0lEgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EarlyStopping: Añadí el callback EarlyStopping para detener el entrenamiento si la validación no mejora después de 3 épocas, lo cual puede ahorrar tiempo."
      ],
      "metadata": {
        "id": "ZamugX5RtkcK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FuTmnT87tlUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduced batch size\n",
        "batch_size = 64 * strategy.num_replicas_in_sync\n",
        "# Crear una instancia de los callbacks\n",
        "time_callback = TimeHistory()\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "# Entrenar el modelo con los callbacks\n",
        "model.fit(xtrain_pad, ytrain_one_hot, epochs=20, batch_size=batch_size, validation_data=(xvalid_pad, yvalid_one_hot), callbacks=[time_callback, early_stopping])\n",
        "\n",
        "#model.fit(xtrain_pad, ytrain_one_hot, epochs=20, batch_size=batch_size, validation_data=(xvalid_pad, yvalid_one_hot))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNE01-i9yEWj",
        "outputId": "74a6ef83-e76e-406b-dd10-578b743dab43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 start time: 2024-07-03 17:13:04.251906\n",
            "Epoch 1/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.6798 - accuracy: 0.5633 Epoch 1 end time: 2024-07-03 17:18:56.855564\n",
            "19/19 [==============================] - 353s 19s/step - loss: 0.6798 - accuracy: 0.5633 - val_loss: 0.7000 - val_accuracy: 0.4900\n",
            "Epoch 2 start time: 2024-07-03 17:18:56.870980\n",
            "Epoch 2/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.6704 - accuracy: 0.5825 Epoch 2 end time: 2024-07-03 17:24:24.014081\n",
            "19/19 [==============================] - 327s 17s/step - loss: 0.6704 - accuracy: 0.5825 - val_loss: 0.7071 - val_accuracy: 0.5100\n",
            "Epoch 3 start time: 2024-07-03 17:24:24.024074\n",
            "Epoch 3/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.6556 - accuracy: 0.6250 Epoch 3 end time: 2024-07-03 17:29:51.679013\n",
            "19/19 [==============================] - 328s 17s/step - loss: 0.6556 - accuracy: 0.6250 - val_loss: 0.7141 - val_accuracy: 0.4800\n",
            "Epoch 4 start time: 2024-07-03 17:29:51.689144\n",
            "Epoch 4/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.6308 - accuracy: 0.6600 Epoch 4 end time: 2024-07-03 17:35:21.687694\n",
            "19/19 [==============================] - 330s 17s/step - loss: 0.6308 - accuracy: 0.6600 - val_loss: 0.7334 - val_accuracy: 0.4750\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7dca585db7f0>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i-8g7N9ecyWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "13:52\n",
        "2:05\n",
        "15 min por epoch"
      ],
      "metadata": {
        "id": "SoLYSzRgg4A6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predecir en el conjunto de validación\n",
        "preds = np.argmax(model.predict(xvalid_pad), axis=-1)\n",
        "\n",
        "# Calcular la precisión\n",
        "accuracy = metrics.accuracy_score(yvalid_encoded, preds)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Generar el reporte de clasificación\n",
        "target_names = label_encoder.classes_  # Esto debería ser ['negative', 'positive']\n",
        "clas_report = classification_report(yvalid_encoded, preds, target_names=target_names)\n",
        "print(clas_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gq1E_HSpwJya",
        "outputId": "940062c9-68da-4a34-e71e-4fada92ff28f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 20s 2s/step\n",
            "Accuracy: 0.475\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.47      0.47      0.48       200\n",
            "    positive       0.47      0.47      0.48       200\n",
            "\n",
            "    accuracy                           0.48       400\n",
            "   macro avg       0.47      0.47      0.48       400\n",
            "weighted avg       0.47      0.47      0.47       400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resultados de Modelos de Clasificación de Reviews de Películas\n",
        "\n",
        "| Modelo                       | Precisión Negativo | Recall Negativo | F1-Score Negativo | Precisión Positivo | Recall Positivo | F1-Score Positivo | Accuracy | Macro Avg F1-Score | Weighted Avg F1-Score |\n",
        "|------------------------------|--------------------|-----------------|-------------------|--------------------|-----------------|-------------------|----------|--------------------|-----------------------|\n",
        "| SimpleRNN                    | 0.49               | 0.69            | 0.57              | 0.48               | 0.29            | 0.37              | 0.49     | 0.47               | 0.47                  |\n",
        "| CNN                          | 0.79               | 0.77            | 0.78              | 0.77               | 0.79            | 0.78              | 0.78     | 0.78               | 0.78                  |\n",
        "| RNN con Embeddings Preentrenados | 0.66               | 0.60            | 0.63              | 0.63               | 0.69            | 0.66              | 0.65     | 0.64               | 0.64                  |\n",
        "| LSTM                         | 0.50               | 1.00            | 0.67              | 0.00               | 0.00            | 0.00              | 0.50     | 0.33               | 0.33                  |\n",
        "| GRU                          | 0.50               | 1.00            | 0.67              | 0.00               | 0.00            | 0.00              | 0.50     | 0.33               | 0.33                  |\n",
        "| Bidirectional LSTM           | 0.47               | 0.47            | 0.48              | 0.47               | 0.47            | 0.48              | 0.48     | 0.48               | 0.47                  |\n",
        "\n",
        "## Conclusiones\n",
        "\n",
        "1. **Modelo CNN**:\n",
        "   - **Mejor Rendimiento**: El modelo CNN tiene el mejor rendimiento en todos los aspectos y es el más recomendable para la tarea de clasificación de reviews de películas.\n",
        "\n",
        "2. **Modelo RNN con Embeddings Preentrenados**:\n",
        "   - **Rendimiento Medio**: Tiene un rendimiento intermedio, mejor que los modelos LSTM, GRU y Bidirectional LSTM pero inferior al modelo CNN.\n",
        "\n",
        "3. **Modelos SimpleRNN, LSTM, GRU y Bidirectional LSTM**:\n",
        "   - **Rendimiento Bajo**: Todos estos modelos tienen un rendimiento significativamente menor comparado con el modelo CNN, especialmente en la clase positiva.\n"
      ],
      "metadata": {
        "id": "l2KQcQN4vyIP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interesantes resultados:  Los modelos que utilizan embeddings preenrenados dieron resultados mas bajos y esto es  utilizamos embeddings en español y estos no son  adecuados para clasificar reviews en inglés. Para mejorar el rendimiento de los modelos que utilizan embeddings preentrenados, debería utilizar  embeddings entrenados en inglés."
      ],
      "metadata": {
        "id": "UPKyvgY145KW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Supongamos que `model` es el modelo CNN ya entrenado\n",
        "# y `tokenizer` es el tokenizer ajustado en el conjunto de datos de entrenamiento\n",
        "\n",
        "def preprocesar_review(review, tokenizer, max_len):\n",
        "    # Convertir la review en una secuencia de índices\n",
        "    seq = tokenizer.texts_to_sequences([review])\n",
        "    # Aplicar padding a la secuencia\n",
        "    padded_seq = pad_sequences(seq, maxlen=max_len)\n",
        "    return padded_seq\n",
        "\n",
        "def clasificar_review(review, model, tokenizer, max_len):\n",
        "    # Preprocesar la review\n",
        "    preprocessed_review = preprocesar_review(review, tokenizer, max_len)\n",
        "    # Hacer la predicción\n",
        "    pred = model.predict(preprocessed_review)\n",
        "    # Obtener la clase predicha\n",
        "    pred_class = np.argmax(pred, axis=1)\n",
        "    return 'positive' if pred_class[0] == 1 else 'negative'\n",
        "\n",
        "# Ejemplo de uso:\n",
        "nueva_review = \"This movie was incredible, the acting was spectacular and the plot was very involving.\"\n",
        "resultado = clasificar_review(nueva_review, model, tokenizer, max_len)\n",
        "print(f\"La review es: {resultado}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaciaV-PwI2R",
        "outputId": "8d2964a4-d3b6-4c60-f88b-25c810250591"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 4s 4s/step\n",
            "La review es: positive\n"
          ]
        }
      ]
    }
  ]
}